nohup: ignoring input
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
| distributed init (rank 0, word 4): env://
| distributed init (rank 3, word 4): env://
| distributed init (rank 1, word 4): env://
| distributed init (rank 2, word 4): env://
Creating dataset
loading /home/monajati/main/med/blip/med_data/medical_20k_50per.json
number of training samples: 11397
Creating model
/embeddings/word_embeddings is tied
/embeddings/position_embeddings is tied
/embeddings/LayerNorm is tied
/encoder/layer/0/crossattention/self/query is tied
/encoder/layer/0/crossattention/self/key is tied
/encoder/layer/0/crossattention/self/value is tied
/encoder/layer/0/crossattention/output/dense is tied
/encoder/layer/0/crossattention/output/LayerNorm is tied
/encoder/layer/0/intermediate/dense is tied
/encoder/layer/0/output/dense is tied
/encoder/layer/0/output/LayerNorm is tied
/encoder/layer/1/crossattention/self/query is tied
/encoder/layer/1/crossattention/self/key is tied
/encoder/layer/1/crossattention/self/value is tied
/encoder/layer/1/crossattention/output/dense is tied
/encoder/layer/1/crossattention/output/LayerNorm is tied
/encoder/layer/1/intermediate/dense is tied
/encoder/layer/1/output/dense is tied
/encoder/layer/1/output/LayerNorm is tied
/encoder/layer/2/crossattention/self/query is tied
/encoder/layer/2/crossattention/self/key is tied
/encoder/layer/2/crossattention/self/value is tied
/encoder/layer/2/crossattention/output/dense is tied
/encoder/layer/2/crossattention/output/LayerNorm is tied
/encoder/layer/2/intermediate/dense is tied
/encoder/layer/2/output/dense is tied
/encoder/layer/2/output/LayerNorm is tied
/encoder/layer/3/crossattention/self/query is tied
/encoder/layer/3/crossattention/self/key is tied
/encoder/layer/3/crossattention/self/value is tied
/encoder/layer/3/crossattention/output/dense is tied
/encoder/layer/3/crossattention/output/LayerNorm is tied
/encoder/layer/3/intermediate/dense is tied
/encoder/layer/3/output/dense is tied
/encoder/layer/3/output/LayerNorm is tied
/encoder/layer/4/crossattention/self/query is tied
/encoder/layer/4/crossattention/self/key is tied
/encoder/layer/4/crossattention/self/value is tied
/encoder/layer/4/crossattention/output/dense is tied
/encoder/layer/4/crossattention/output/LayerNorm is tied
/encoder/layer/4/intermediate/dense is tied
/encoder/layer/4/output/dense is tied
/encoder/layer/4/output/LayerNorm is tied
/encoder/layer/5/crossattention/self/query is tied
/encoder/layer/5/crossattention/self/key is tied
/encoder/layer/5/crossattention/self/value is tied
/encoder/layer/5/crossattention/output/dense is tied
/encoder/layer/5/crossattention/output/LayerNorm is tied
/encoder/layer/5/intermediate/dense is tied
/encoder/layer/5/output/dense is tied
/encoder/layer/5/output/LayerNorm is tied
/encoder/layer/6/crossattention/self/query is tied
/encoder/layer/6/crossattention/self/key is tied
/encoder/layer/6/crossattention/self/value is tied
/encoder/layer/6/crossattention/output/dense is tied
/encoder/layer/6/crossattention/output/LayerNorm is tied
/encoder/layer/6/intermediate/dense is tied
/encoder/layer/6/output/dense is tied
/encoder/layer/6/output/LayerNorm is tied
/encoder/layer/7/crossattention/self/query is tied
/encoder/layer/7/crossattention/self/key is tied
/encoder/layer/7/crossattention/self/value is tied
/encoder/layer/7/crossattention/output/dense is tied
/encoder/layer/7/crossattention/output/LayerNorm is tied
/encoder/layer/7/intermediate/dense is tied
/encoder/layer/7/output/dense is tied
/encoder/layer/7/output/LayerNorm is tied
/encoder/layer/8/crossattention/self/query is tied
/encoder/layer/8/crossattention/self/key is tied
/encoder/layer/8/crossattention/self/value is tied
/encoder/layer/8/crossattention/output/dense is tied
/encoder/layer/8/crossattention/output/LayerNorm is tied
/encoder/layer/8/intermediate/dense is tied
/encoder/layer/8/output/dense is tied
/encoder/layer/8/output/LayerNorm is tied
/encoder/layer/9/crossattention/self/query is tied
/encoder/layer/9/crossattention/self/key is tied
/encoder/layer/9/crossattention/self/value is tied
/encoder/layer/9/crossattention/output/dense is tied
/encoder/layer/9/crossattention/output/LayerNorm is tied
/encoder/layer/9/intermediate/dense is tied
/encoder/layer/9/output/dense is tied
/encoder/layer/9/output/LayerNorm is tied
/encoder/layer/10/crossattention/self/query is tied
/encoder/layer/10/crossattention/self/key is tied
/encoder/layer/10/crossattention/self/value is tied
/encoder/layer/10/crossattention/output/dense is tied
/encoder/layer/10/crossattention/output/LayerNorm is tied
/encoder/layer/10/intermediate/dense is tied
/encoder/layer/10/output/dense is tied
/encoder/layer/10/output/LayerNorm is tied
/encoder/layer/11/crossattention/self/query is tied
/encoder/layer/11/crossattention/self/key is tied
/encoder/layer/11/crossattention/self/value is tied
/encoder/layer/11/crossattention/output/dense is tied
/encoder/layer/11/crossattention/output/LayerNorm is tied
/encoder/layer/11/intermediate/dense is tied
/encoder/layer/11/output/dense is tied
/encoder/layer/11/output/LayerNorm is tied
resume checkpoint from /home/monajati/main/med/blip/model_base.pth
args.gpu 0
Start training
Train Epoch: [0]  [  0/285]  eta: 1:32:36  lr: 0.000001  loss_ita: 5.3354  loss_itm: 1.5886  loss_lm: 9.3456  time: 19.4950  data: 0.6884  max mem: 19699
Train Epoch: [0]  [ 50/285]  eta: 0:04:27  lr: 0.000006  loss_ita: 5.9348  loss_itm: 0.6251  loss_lm: 6.5587  time: 0.6271  data: 0.0001  max mem: 22679
Train Epoch: [0]  [100/285]  eta: 0:02:43  lr: 0.000011  loss_ita: 4.9696  loss_itm: 0.5315  loss_lm: 6.2686  time: 0.6312  data: 0.0002  max mem: 22679
Train Epoch: [0]  [150/285]  eta: 0:01:48  lr: 0.000015  loss_ita: 5.4827  loss_itm: 0.6240  loss_lm: 5.8368  time: 0.6325  data: 0.0001  max mem: 22679
Train Epoch: [0]  [200/285]  eta: 0:01:04  lr: 0.000020  loss_ita: 4.9794  loss_itm: 0.5016  loss_lm: 5.5024  time: 0.6334  data: 0.0002  max mem: 22679
Train Epoch: [0]  [250/285]  eta: 0:00:25  lr: 0.000025  loss_ita: 4.9514  loss_itm: 0.5284  loss_lm: 5.2262  time: 0.6336  data: 0.0002  max mem: 22679
Train Epoch: [0]  [284/285]  eta: 0:00:00  lr: 0.000028  loss_ita: 4.8741  loss_itm: 0.4710  loss_lm: 5.0342  time: 0.6314  data: 0.0001  max mem: 22679
Train Epoch: [0] Total time: 0:03:26 (0.7234 s / it)
Averaged stats: lr: 0.0000  loss_ita: 5.4456  loss_itm: 0.6294  loss_lm: 5.9041
Train Epoch: [1]  [  0/285]  eta: 0:05:50  lr: 0.000027  loss_ita: 4.2920  loss_itm: 0.5388  loss_lm: 4.9148  time: 1.2314  data: 0.5395  max mem: 22679
Train Epoch: [1]  [ 50/285]  eta: 0:02:31  lr: 0.000027  loss_ita: 4.4885  loss_itm: 0.5537  loss_lm: 5.1411  time: 0.6334  data: 0.0001  max mem: 22679
Train Epoch: [1]  [100/285]  eta: 0:01:58  lr: 0.000027  loss_ita: 5.1811  loss_itm: 0.4499  loss_lm: 5.1696  time: 0.6336  data: 0.0001  max mem: 22679
Train Epoch: [1]  [150/285]  eta: 0:01:26  lr: 0.000027  loss_ita: 5.6340  loss_itm: 0.5291  loss_lm: 4.6768  time: 0.6354  data: 0.0001  max mem: 22679
Train Epoch: [1]  [200/285]  eta: 0:00:54  lr: 0.000027  loss_ita: 5.3220  loss_itm: 0.5128  loss_lm: 4.6108  time: 0.6328  data: 0.0001  max mem: 22679
Train Epoch: [1]  [250/285]  eta: 0:00:22  lr: 0.000027  loss_ita: 5.4510  loss_itm: 0.6489  loss_lm: 4.9141  time: 0.6337  data: 0.0001  max mem: 22679
Train Epoch: [1]  [284/285]  eta: 0:00:00  lr: 0.000027  loss_ita: 5.1278  loss_itm: 0.4694  loss_lm: 4.8155  time: 0.6326  data: 0.0001  max mem: 22679
Train Epoch: [1] Total time: 0:03:01 (0.6372 s / it)
Averaged stats: lr: 0.0000  loss_ita: 5.3552  loss_itm: 0.5451  loss_lm: 4.7794
Train Epoch: [2]  [  0/285]  eta: 0:06:17  lr: 0.000024  loss_ita: 5.1395  loss_itm: 0.4380  loss_lm: 4.9013  time: 1.3258  data: 0.5544  max mem: 22679
Train Epoch: [2]  [ 50/285]  eta: 0:02:32  lr: 0.000024  loss_ita: 5.2722  loss_itm: 0.4781  loss_lm: 4.3750  time: 0.6326  data: 0.0001  max mem: 22679
Train Epoch: [2]  [100/285]  eta: 0:01:58  lr: 0.000024  loss_ita: 5.7456  loss_itm: 0.5408  loss_lm: 4.2588  time: 0.6365  data: 0.0001  max mem: 22679
Train Epoch: [2]  [150/285]  eta: 0:01:26  lr: 0.000024  loss_ita: 5.4623  loss_itm: 0.4524  loss_lm: 4.4450  time: 0.6381  data: 0.0001  max mem: 22679
Train Epoch: [2]  [200/285]  eta: 0:00:54  lr: 0.000024  loss_ita: 5.1210  loss_itm: 0.4728  loss_lm: 4.6110  time: 0.6319  data: 0.0001  max mem: 22679
Train Epoch: [2]  [250/285]  eta: 0:00:22  lr: 0.000024  loss_ita: 5.4048  loss_itm: 0.6468  loss_lm: 4.0879  time: 0.6341  data: 0.0001  max mem: 22679
Train Epoch: [2]  [284/285]  eta: 0:00:00  lr: 0.000024  loss_ita: 5.0816  loss_itm: 0.4068  loss_lm: 4.4793  time: 0.6342  data: 0.0001  max mem: 22679
Train Epoch: [2] Total time: 0:03:01 (0.6383 s / it)
Averaged stats: lr: 0.0000  loss_ita: 5.4546  loss_itm: 0.4952  loss_lm: 4.4510
Train Epoch: [3]  [  0/285]  eta: 0:06:19  lr: 0.000022  loss_ita: 5.8315  loss_itm: 0.6242  loss_lm: 3.9655  time: 1.3300  data: 0.5598  max mem: 22679
Train Epoch: [3]  [ 50/285]  eta: 0:02:32  lr: 0.000022  loss_ita: 4.4782  loss_itm: 0.4355  loss_lm: 4.8128  time: 0.6335  data: 0.0001  max mem: 22679
Train Epoch: [3]  [100/285]  eta: 0:01:58  lr: 0.000022  loss_ita: 5.3954  loss_itm: 0.4388  loss_lm: 4.6822  time: 0.6347  data: 0.0001  max mem: 22679
Train Epoch: [3]  [150/285]  eta: 0:01:26  lr: 0.000022  loss_ita: 5.1942  loss_itm: 0.5406  loss_lm: 4.7377  time: 0.6335  data: 0.0001  max mem: 22679
Train Epoch: [3]  [200/285]  eta: 0:00:54  lr: 0.000022  loss_ita: 4.5864  loss_itm: 0.3702  loss_lm: 4.1021  time: 0.6355  data: 0.0001  max mem: 22679
Train Epoch: [3]  [250/285]  eta: 0:00:22  lr: 0.000022  loss_ita: 3.8094  loss_itm: 0.4016  loss_lm: 4.7041  time: 0.6340  data: 0.0001  max mem: 22679
Train Epoch: [3]  [284/285]  eta: 0:00:00  lr: 0.000022  loss_ita: 4.8212  loss_itm: 0.4503  loss_lm: 3.7996  time: 0.6307  data: 0.0001  max mem: 22679
Train Epoch: [3] Total time: 0:03:01 (0.6375 s / it)
Averaged stats: lr: 0.0000  loss_ita: 5.1261  loss_itm: 0.4658  loss_lm: 4.2660
Train Epoch: [4]  [  0/285]  eta: 0:06:20  lr: 0.000020  loss_ita: 5.1649  loss_itm: 0.3778  loss_lm: 4.2607  time: 1.3338  data: 0.5309  max mem: 22679
Train Epoch: [4]  [ 50/285]  eta: 0:02:33  lr: 0.000020  loss_ita: 5.6773  loss_itm: 0.4662  loss_lm: 4.3131  time: 0.6418  data: 0.0001  max mem: 22679
Train Epoch: [4]  [100/285]  eta: 0:01:58  lr: 0.000020  loss_ita: 5.3172  loss_itm: 0.4144  loss_lm: 4.6137  time: 0.6338  data: 0.0001  max mem: 22679
Train Epoch: [4]  [150/285]  eta: 0:01:26  lr: 0.000020  loss_ita: 4.2633  loss_itm: 0.2993  loss_lm: 4.2177  time: 0.6325  data: 0.0001  max mem: 22679
Train Epoch: [4]  [200/285]  eta: 0:00:54  lr: 0.000020  loss_ita: 5.6414  loss_itm: 0.5269  loss_lm: 3.8240  time: 0.6325  data: 0.0001  max mem: 22679
Train Epoch: [4]  [250/285]  eta: 0:00:22  lr: 0.000020  loss_ita: 5.2609  loss_itm: 0.5369  loss_lm: 3.6818  time: 0.6324  data: 0.0001  max mem: 22679
Train Epoch: [4]  [284/285]  eta: 0:00:00  lr: 0.000020  loss_ita: 5.2605  loss_itm: 0.6168  loss_lm: 3.8152  time: 0.6322  data: 0.0001  max mem: 22679
Train Epoch: [4] Total time: 0:03:01 (0.6367 s / it)
Averaged stats: lr: 0.0000  loss_ita: 5.0832  loss_itm: 0.4433  loss_lm: 4.1449
Train Epoch: [5]  [  0/285]  eta: 0:10:46  lr: 0.000018  loss_ita: 5.2922  loss_itm: 0.4386  loss_lm: 3.6166  time: 2.2669  data: 0.5133  max mem: 23531
Train Epoch: [5]  [ 50/285]  eta: 0:02:36  lr: 0.000018  loss_ita: 4.0067  loss_itm: 0.3003  loss_lm: 4.0939  time: 0.6307  data: 0.0001  max mem: 23559
Train Epoch: [5]  [100/285]  eta: 0:01:59  lr: 0.000018  loss_ita: 4.8263  loss_itm: 0.5726  loss_lm: 4.2461  time: 0.6311  data: 0.0001  max mem: 23560
Train Epoch: [5]  [150/285]  eta: 0:01:26  lr: 0.000018  loss_ita: 5.9023  loss_itm: 0.6582  loss_lm: 3.9594  time: 0.6299  data: 0.0001  max mem: 23562
Train Epoch: [5]  [200/285]  eta: 0:00:54  lr: 0.000018  loss_ita: 4.6413  loss_itm: 0.4689  loss_lm: 4.3501  time: 0.6305  data: 0.0001  max mem: 23567
Train Epoch: [5]  [250/285]  eta: 0:00:22  lr: 0.000018  loss_ita: 4.9135  loss_itm: 0.4150  loss_lm: 3.4878  time: 0.6293  data: 0.0001  max mem: 23570
Train Epoch: [5]  [284/285]  eta: 0:00:00  lr: 0.000018  loss_ita: 4.8999  loss_itm: 0.5709  loss_lm: 3.7610  time: 0.6300  data: 0.0001  max mem: 23573
Train Epoch: [5] Total time: 0:03:01 (0.6376 s / it)
Averaged stats: lr: 0.0000  loss_ita: 4.8154  loss_itm: 0.4139  loss_lm: 4.0374
Train Epoch: [6]  [  0/285]  eta: 0:06:07  lr: 0.000016  loss_ita: 4.8917  loss_itm: 0.3826  loss_lm: 3.7415  time: 1.2910  data: 0.5282  max mem: 23573
Train Epoch: [6]  [ 50/285]  eta: 0:02:31  lr: 0.000016  loss_ita: 4.0300  loss_itm: 0.2456  loss_lm: 4.2858  time: 0.6325  data: 0.0001  max mem: 23573
Train Epoch: [6]  [100/285]  eta: 0:01:58  lr: 0.000016  loss_ita: 4.7245  loss_itm: 0.3246  loss_lm: 3.9821  time: 0.6293  data: 0.0001  max mem: 23573
Train Epoch: [6]  [150/285]  eta: 0:01:25  lr: 0.000016  loss_ita: 5.2380  loss_itm: 0.6505  loss_lm: 3.6490  time: 0.6308  data: 0.0001  max mem: 23573
Train Epoch: [6]  [200/285]  eta: 0:00:53  lr: 0.000016  loss_ita: 4.6910  loss_itm: 0.5256  loss_lm: 3.6135  time: 0.6311  data: 0.0001  max mem: 23573
Train Epoch: [6]  [250/285]  eta: 0:00:22  lr: 0.000016  loss_ita: 3.4768  loss_itm: 0.2369  loss_lm: 3.6252  time: 0.6296  data: 0.0001  max mem: 23573
Train Epoch: [6]  [284/285]  eta: 0:00:00  lr: 0.000016  loss_ita: 3.9064  loss_itm: 0.3083  loss_lm: 4.0779  time: 0.6292  data: 0.0001  max mem: 23573
Train Epoch: [6] Total time: 0:03:00 (0.6343 s / it)
Averaged stats: lr: 0.0000  loss_ita: 4.4008  loss_itm: 0.3836  loss_lm: 3.9649
Train Epoch: [7]  [  0/285]  eta: 0:06:04  lr: 0.000014  loss_ita: 3.7822  loss_itm: 0.2567  loss_lm: 4.1900  time: 1.2803  data: 0.5065  max mem: 23573
Train Epoch: [7]  [ 50/285]  eta: 0:02:31  lr: 0.000014  loss_ita: 2.6508  loss_itm: 0.1018  loss_lm: 4.6169  time: 0.6320  data: 0.0001  max mem: 23573
Train Epoch: [7]  [100/285]  eta: 0:01:57  lr: 0.000014  loss_ita: 4.3540  loss_itm: 0.3827  loss_lm: 4.1723  time: 0.6301  data: 0.0001  max mem: 23573
Train Epoch: [7]  [150/285]  eta: 0:01:25  lr: 0.000014  loss_ita: 3.3904  loss_itm: 0.3718  loss_lm: 4.0242  time: 0.6299  data: 0.0001  max mem: 23573
Train Epoch: [7]  [200/285]  eta: 0:00:53  lr: 0.000014  loss_ita: 4.6698  loss_itm: 0.3870  loss_lm: 3.6100  time: 0.6294  data: 0.0001  max mem: 23577
Train Epoch: [7]  [250/285]  eta: 0:00:22  lr: 0.000014  loss_ita: 3.6663  loss_itm: 0.2520  loss_lm: 3.7122  time: 0.6305  data: 0.0001  max mem: 23577
Train Epoch: [7]  [284/285]  eta: 0:00:00  lr: 0.000014  loss_ita: 3.8457  loss_itm: 0.4851  loss_lm: 4.3230  time: 0.6291  data: 0.0001  max mem: 23577
Train Epoch: [7] Total time: 0:03:00 (0.6338 s / it)
Averaged stats: lr: 0.0000  loss_ita: 3.8557  loss_itm: 0.3586  loss_lm: 3.8965
Train Epoch: [8]  [  0/285]  eta: 0:06:15  lr: 0.000013  loss_ita: 4.4188  loss_itm: 0.4118  loss_lm: 4.4237  time: 1.3169  data: 0.5489  max mem: 23577
Train Epoch: [8]  [ 50/285]  eta: 0:02:31  lr: 0.000013  loss_ita: 3.6320  loss_itm: 0.2056  loss_lm: 4.4273  time: 0.6312  data: 0.0001  max mem: 23577
Train Epoch: [8]  [100/285]  eta: 0:01:58  lr: 0.000013  loss_ita: 3.4627  loss_itm: 0.2922  loss_lm: 3.7923  time: 0.6342  data: 0.0001  max mem: 23577
Train Epoch: [8]  [150/285]  eta: 0:01:25  lr: 0.000013  loss_ita: 4.1953  loss_itm: 0.4458  loss_lm: 4.5124  time: 0.6309  data: 0.0001  max mem: 23577
Train Epoch: [8]  [200/285]  eta: 0:00:53  lr: 0.000013  loss_ita: 4.4450  loss_itm: 0.3342  loss_lm: 3.8459  time: 0.6323  data: 0.0001  max mem: 23577
Train Epoch: [8]  [250/285]  eta: 0:00:22  lr: 0.000013  loss_ita: 3.8794  loss_itm: 0.3080  loss_lm: 3.8100  time: 0.6309  data: 0.0001  max mem: 23577
Train Epoch: [8]  [284/285]  eta: 0:00:00  lr: 0.000013  loss_ita: 4.3024  loss_itm: 0.5522  loss_lm: 3.8153  time: 0.6322  data: 0.0001  max mem: 23577
Train Epoch: [8] Total time: 0:03:00 (0.6346 s / it)
Averaged stats: lr: 0.0000  loss_ita: 3.8162  loss_itm: 0.3385  loss_lm: 3.8486
Train Epoch: [9]  [  0/285]  eta: 0:06:10  lr: 0.000012  loss_ita: 4.7521  loss_itm: 0.4513  loss_lm: 3.5138  time: 1.2983  data: 0.5356  max mem: 23577
Train Epoch: [9]  [ 50/285]  eta: 0:02:31  lr: 0.000012  loss_ita: 3.4235  loss_itm: 0.1499  loss_lm: 3.4744  time: 0.6320  data: 0.0001  max mem: 23577
Train Epoch: [9]  [100/285]  eta: 0:01:57  lr: 0.000012  loss_ita: 3.4015  loss_itm: 0.1487  loss_lm: 3.6489  time: 0.6305  data: 0.0001  max mem: 23577
Train Epoch: [9]  [150/285]  eta: 0:01:25  lr: 0.000012  loss_ita: 3.2129  loss_itm: 0.1635  loss_lm: 4.1447  time: 0.6292  data: 0.0001  max mem: 23577
Train Epoch: [9]  [200/285]  eta: 0:00:53  lr: 0.000012  loss_ita: 4.1918  loss_itm: 0.3415  loss_lm: 3.8741  time: 0.6300  data: 0.0001  max mem: 23577
Train Epoch: [9]  [250/285]  eta: 0:00:22  lr: 0.000012  loss_ita: 3.5842  loss_itm: 0.2042  loss_lm: 3.8440  time: 0.6308  data: 0.0001  max mem: 23577
Train Epoch: [9]  [284/285]  eta: 0:00:00  lr: 0.000012  loss_ita: 3.6959  loss_itm: 0.1754  loss_lm: 3.8017  time: 0.6336  data: 0.0001  max mem: 23577
Train Epoch: [9] Total time: 0:03:00 (0.6337 s / it)
Averaged stats: lr: 0.0000  loss_ita: 3.5454  loss_itm: 0.3285  loss_lm: 3.8024
Train Epoch: [10]  [  0/285]  eta: 0:06:13  lr: 0.000010  loss_ita: 4.2287  loss_itm: 0.4422  loss_lm: 3.6155  time: 1.3112  data: 0.4919  max mem: 23577
Train Epoch: [10]  [ 50/285]  eta: 0:02:31  lr: 0.000010  loss_ita: 2.8009  loss_itm: 0.1015  loss_lm: 3.6925  time: 0.6322  data: 0.0001  max mem: 23577
Train Epoch: [10]  [100/285]  eta: 0:01:57  lr: 0.000010  loss_ita: 3.3694  loss_itm: 0.3125  loss_lm: 3.8515  time: 0.6308  data: 0.0001  max mem: 23577
Train Epoch: [10]  [150/285]  eta: 0:01:25  lr: 0.000010  loss_ita: 3.7829  loss_itm: 0.2556  loss_lm: 3.4587  time: 0.6302  data: 0.0001  max mem: 23577
Train Epoch: [10]  [200/285]  eta: 0:00:53  lr: 0.000010  loss_ita: 4.5230  loss_itm: 0.3967  loss_lm: 3.6325  time: 0.6312  data: 0.0001  max mem: 23577
Train Epoch: [10]  [250/285]  eta: 0:00:22  lr: 0.000010  loss_ita: 3.7844  loss_itm: 0.2613  loss_lm: 3.7744  time: 0.6306  data: 0.0001  max mem: 23577
Train Epoch: [10]  [284/285]  eta: 0:00:00  lr: 0.000010  loss_ita: 3.1216  loss_itm: 0.1819  loss_lm: 3.5201  time: 0.6298  data: 0.0001  max mem: 23577
Train Epoch: [10] Total time: 0:03:00 (0.6343 s / it)
Averaged stats: lr: 0.0000  loss_ita: 3.8529  loss_itm: 0.3066  loss_lm: 3.7610
Train Epoch: [11]  [  0/285]  eta: 0:06:08  lr: 0.000009  loss_ita: 3.5920  loss_itm: 0.2881  loss_lm: 3.3857  time: 1.2940  data: 0.4958  max mem: 23577
Train Epoch: [11]  [ 50/285]  eta: 0:02:31  lr: 0.000009  loss_ita: 4.3058  loss_itm: 0.3420  loss_lm: 3.5688  time: 0.6309  data: 0.0001  max mem: 23577
Train Epoch: [11]  [100/285]  eta: 0:01:57  lr: 0.000009  loss_ita: 4.0099  loss_itm: 0.2823  loss_lm: 3.3179  time: 0.6292  data: 0.0001  max mem: 23577
Train Epoch: [11]  [150/285]  eta: 0:01:25  lr: 0.000009  loss_ita: 2.8076  loss_itm: 0.1488  loss_lm: 3.8640  time: 0.6291  data: 0.0001  max mem: 23577
Train Epoch: [11]  [200/285]  eta: 0:00:53  lr: 0.000009  loss_ita: 4.3439  loss_itm: 0.2793  loss_lm: 4.0450  time: 0.6296  data: 0.0001  max mem: 23577
Train Epoch: [11]  [250/285]  eta: 0:00:22  lr: 0.000009  loss_ita: 4.2414  loss_itm: 0.3102  loss_lm: 3.6262  time: 0.6294  data: 0.0001  max mem: 23577
Train Epoch: [11]  [284/285]  eta: 0:00:00  lr: 0.000009  loss_ita: 3.5877  loss_itm: 0.2238  loss_lm: 3.6973  time: 0.6303  data: 0.0001  max mem: 23577
Train Epoch: [11] Total time: 0:03:00 (0.6335 s / it)
Averaged stats: lr: 0.0000  loss_ita: 3.9850  loss_itm: 0.2879  loss_lm: 3.7253
Train Epoch: [12]  [  0/285]  eta: 0:06:06  lr: 0.000008  loss_ita: 4.0789  loss_itm: 0.1800  loss_lm: 3.8651  time: 1.2864  data: 0.5146  max mem: 23577
Train Epoch: [12]  [ 50/285]  eta: 0:02:31  lr: 0.000008  loss_ita: 4.0747  loss_itm: 0.3192  loss_lm: 3.3106  time: 0.6333  data: 0.0001  max mem: 23577
Train Epoch: [12]  [100/285]  eta: 0:01:58  lr: 0.000008  loss_ita: 3.9861  loss_itm: 0.4535  loss_lm: 3.3725  time: 0.6304  data: 0.0001  max mem: 23577
Train Epoch: [12]  [150/285]  eta: 0:01:25  lr: 0.000008  loss_ita: 3.3574  loss_itm: 0.2617  loss_lm: 3.7278  time: 0.6388  data: 0.0001  max mem: 23577
Train Epoch: [12]  [200/285]  eta: 0:00:53  lr: 0.000008  loss_ita: 2.6685  loss_itm: 0.1107  loss_lm: 3.9199  time: 0.6305  data: 0.0001  max mem: 23577
Train Epoch: [12]  [250/285]  eta: 0:00:22  lr: 0.000008  loss_ita: 3.1006  loss_itm: 0.2456  loss_lm: 3.6471  time: 0.6306  data: 0.0001  max mem: 23577
Train Epoch: [12]  [284/285]  eta: 0:00:00  lr: 0.000008  loss_ita: 3.6952  loss_itm: 0.2752  loss_lm: 3.7306  time: 0.6290  data: 0.0001  max mem: 23577
Train Epoch: [12] Total time: 0:03:00 (0.6340 s / it)
Averaged stats: lr: 0.0000  loss_ita: 3.6026  loss_itm: 0.2749  loss_lm: 3.6965
Train Epoch: [13]  [  0/285]  eta: 0:06:05  lr: 0.000008  loss_ita: 3.8388  loss_itm: 0.4551  loss_lm: 3.2920  time: 1.2839  data: 0.5245  max mem: 23577
Train Epoch: [13]  [ 50/285]  eta: 0:02:31  lr: 0.000008  loss_ita: 3.3865  loss_itm: 0.2078  loss_lm: 3.6914  time: 0.6340  data: 0.0001  max mem: 23577
Train Epoch: [13]  [100/285]  eta: 0:01:58  lr: 0.000008  loss_ita: 2.9944  loss_itm: 0.1745  loss_lm: 3.7392  time: 0.6312  data: 0.0001  max mem: 23577
Train Epoch: [13]  [150/285]  eta: 0:01:25  lr: 0.000008  loss_ita: 3.6147  loss_itm: 0.2063  loss_lm: 3.5887  time: 0.6299  data: 0.0001  max mem: 23577
Train Epoch: [13]  [200/285]  eta: 0:00:53  lr: 0.000008  loss_ita: 3.6172  loss_itm: 0.2125  loss_lm: 4.0492  time: 0.6303  data: 0.0001  max mem: 23577
Train Epoch: [13]  [250/285]  eta: 0:00:22  lr: 0.000008  loss_ita: 3.8136  loss_itm: 0.2985  loss_lm: 3.6711  time: 0.6317  data: 0.0001  max mem: 23577
Train Epoch: [13]  [284/285]  eta: 0:00:00  lr: 0.000008  loss_ita: 3.7351  loss_itm: 0.2807  loss_lm: 3.4301  time: 0.6306  data: 0.0001  max mem: 23577
Train Epoch: [13] Total time: 0:03:00 (0.6345 s / it)
Averaged stats: lr: 0.0000  loss_ita: 3.4071  loss_itm: 0.2595  loss_lm: 3.6669
Train Epoch: [14]  [  0/285]  eta: 0:06:13  lr: 0.000007  loss_ita: 2.7224  loss_itm: 0.0736  loss_lm: 3.5905  time: 1.3118  data: 0.5236  max mem: 23577
Train Epoch: [14]  [ 50/285]  eta: 0:02:31  lr: 0.000007  loss_ita: 3.3152  loss_itm: 0.1943  loss_lm: 3.3883  time: 0.6335  data: 0.0001  max mem: 23577
Train Epoch: [14]  [100/285]  eta: 0:01:58  lr: 0.000007  loss_ita: 3.0159  loss_itm: 0.3619  loss_lm: 4.4296  time: 0.6313  data: 0.0001  max mem: 23577
Train Epoch: [14]  [150/285]  eta: 0:01:25  lr: 0.000007  loss_ita: 3.8945  loss_itm: 0.2654  loss_lm: 3.7443  time: 0.6303  data: 0.0001  max mem: 23577
Train Epoch: [14]  [200/285]  eta: 0:00:54  lr: 0.000007  loss_ita: 4.0497  loss_itm: 0.2827  loss_lm: 3.6918  time: 0.6323  data: 0.0001  max mem: 23577
Train Epoch: [14]  [250/285]  eta: 0:00:22  lr: 0.000007  loss_ita: 3.5296  loss_itm: 0.2092  loss_lm: 3.3478  time: 0.6321  data: 0.0001  max mem: 23577
Train Epoch: [14]  [284/285]  eta: 0:00:00  lr: 0.000007  loss_ita: 2.7338  loss_itm: 0.1781  loss_lm: 3.7944  time: 0.6298  data: 0.0001  max mem: 23577
Train Epoch: [14] Total time: 0:03:01 (0.6352 s / it)
Averaged stats: lr: 0.0000  loss_ita: 3.4367  loss_itm: 0.2486  loss_lm: 3.6473
Train Epoch: [15]  [  0/285]  eta: 0:06:14  lr: 0.000006  loss_ita: 2.4990  loss_itm: 0.0721  loss_lm: 3.8493  time: 1.3128  data: 0.5071  max mem: 23577
Train Epoch: [15]  [ 50/285]  eta: 0:02:31  lr: 0.000006  loss_ita: 4.1111  loss_itm: 0.3506  loss_lm: 3.7023  time: 0.6302  data: 0.0001  max mem: 23577
Train Epoch: [15]  [100/285]  eta: 0:01:57  lr: 0.000006  loss_ita: 4.3034  loss_itm: 0.1715  loss_lm: 3.5548  time: 0.6307  data: 0.0001  max mem: 23577
Train Epoch: [15]  [150/285]  eta: 0:01:25  lr: 0.000006  loss_ita: 2.6035  loss_itm: 0.0487  loss_lm: 3.4671  time: 0.6323  data: 0.0001  max mem: 23577
Train Epoch: [15]  [200/285]  eta: 0:00:53  lr: 0.000006  loss_ita: 3.3058  loss_itm: 0.2319  loss_lm: 3.5672  time: 0.6307  data: 0.0001  max mem: 23577
Train Epoch: [15]  [250/285]  eta: 0:00:22  lr: 0.000006  loss_ita: 4.4868  loss_itm: 0.2781  loss_lm: 3.6014  time: 0.6293  data: 0.0001  max mem: 23577
Train Epoch: [15]  [284/285]  eta: 0:00:00  lr: 0.000006  loss_ita: 2.0145  loss_itm: 0.0268  loss_lm: 3.7889  time: 0.6310  data: 0.0001  max mem: 23577
Train Epoch: [15] Total time: 0:03:00 (0.6344 s / it)
Averaged stats: lr: 0.0000  loss_ita: 3.3620  loss_itm: 0.2387  loss_lm: 3.6280
Train Epoch: [16]  [  0/285]  eta: 0:06:06  lr: 0.000006  loss_ita: 2.3650  loss_itm: 0.2415  loss_lm: 3.8247  time: 1.2855  data: 0.5318  max mem: 23577
Train Epoch: [16]  [ 50/285]  eta: 0:02:31  lr: 0.000006  loss_ita: 2.8923  loss_itm: 0.2568  loss_lm: 3.6749  time: 0.6310  data: 0.0001  max mem: 23577
Train Epoch: [16]  [100/285]  eta: 0:01:57  lr: 0.000006  loss_ita: 3.7354  loss_itm: 0.1692  loss_lm: 3.3864  time: 0.6305  data: 0.0001  max mem: 23580
Train Epoch: [16]  [150/285]  eta: 0:01:25  lr: 0.000006  loss_ita: 3.0514  loss_itm: 0.0750  loss_lm: 3.6028  time: 0.6311  data: 0.0001  max mem: 23580
Train Epoch: [16]  [200/285]  eta: 0:00:53  lr: 0.000006  loss_ita: 2.4618  loss_itm: 0.2007  loss_lm: 3.8773  time: 0.6401  data: 0.0001  max mem: 23580
Train Epoch: [16]  [250/285]  eta: 0:00:22  lr: 0.000006  loss_ita: 3.1874  loss_itm: 0.0976  loss_lm: 3.4005  time: 0.6298  data: 0.0001  max mem: 23580
Train Epoch: [16]  [284/285]  eta: 0:00:00  lr: 0.000006  loss_ita: 1.9777  loss_itm: 0.0326  loss_lm: 3.3537  time: 0.6299  data: 0.0001  max mem: 23580
Train Epoch: [16] Total time: 0:03:00 (0.6342 s / it)
Averaged stats: lr: 0.0000  loss_ita: 3.1728  loss_itm: 0.2258  loss_lm: 3.6096
Train Epoch: [17]  [  0/285]  eta: 0:06:06  lr: 0.000005  loss_ita: 2.4994  loss_itm: 0.0650  loss_lm: 3.6491  time: 1.2865  data: 0.5142  max mem: 23580
Train Epoch: [17]  [ 50/285]  eta: 0:02:31  lr: 0.000005  loss_ita: 2.2565  loss_itm: 0.0432  loss_lm: 3.5687  time: 0.6318  data: 0.0001  max mem: 23580
Train Epoch: [17]  [100/285]  eta: 0:01:58  lr: 0.000005  loss_ita: 3.1275  loss_itm: 0.1505  loss_lm: 3.9187  time: 0.6321  data: 0.0001  max mem: 23580
Train Epoch: [17]  [150/285]  eta: 0:01:25  lr: 0.000005  loss_ita: 2.6568  loss_itm: 0.1931  loss_lm: 3.8043  time: 0.6295  data: 0.0001  max mem: 23580
Train Epoch: [17]  [200/285]  eta: 0:00:53  lr: 0.000005  loss_ita: 2.7675  loss_itm: 0.1461  loss_lm: 3.7289  time: 0.6327  data: 0.0001  max mem: 23580
Train Epoch: [17]  [250/285]  eta: 0:00:22  lr: 0.000005  loss_ita: 2.2683  loss_itm: 0.1101  loss_lm: 3.3893  time: 0.6310  data: 0.0001  max mem: 23580
Train Epoch: [17]  [284/285]  eta: 0:00:00  lr: 0.000005  loss_ita: 3.1252  loss_itm: 0.2274  loss_lm: 3.1467  time: 0.6308  data: 0.0001  max mem: 23580
Train Epoch: [17] Total time: 0:03:00 (0.6346 s / it)
Averaged stats: lr: 0.0000  loss_ita: 3.0294  loss_itm: 0.2112  loss_lm: 3.5906
Train Epoch: [18]  [  0/285]  eta: 0:06:06  lr: 0.000005  loss_ita: 3.4845  loss_itm: 0.1388  loss_lm: 3.2450  time: 1.2856  data: 0.5036  max mem: 23580
Train Epoch: [18]  [ 50/285]  eta: 0:02:31  lr: 0.000005  loss_ita: 1.7397  loss_itm: 0.1671  loss_lm: 3.4374  time: 0.6325  data: 0.0001  max mem: 23580
Train Epoch: [18]  [100/285]  eta: 0:01:58  lr: 0.000005  loss_ita: 3.6004  loss_itm: 0.3243  loss_lm: 3.2832  time: 0.6326  data: 0.0001  max mem: 23580
Train Epoch: [18]  [150/285]  eta: 0:01:25  lr: 0.000005  loss_ita: 2.5627  loss_itm: 0.0869  loss_lm: 3.7695  time: 0.6319  data: 0.0001  max mem: 23580
Train Epoch: [18]  [200/285]  eta: 0:00:53  lr: 0.000005  loss_ita: 2.5528  loss_itm: 0.0829  loss_lm: 3.5931  time: 0.6306  data: 0.0001  max mem: 23580
Train Epoch: [18]  [250/285]  eta: 0:00:22  lr: 0.000005  loss_ita: 3.2579  loss_itm: 0.2048  loss_lm: 3.4311  time: 0.6297  data: 0.0001  max mem: 23580
Train Epoch: [18]  [284/285]  eta: 0:00:00  lr: 0.000005  loss_ita: 2.6790  loss_itm: 0.0811  loss_lm: 3.7045  time: 0.6299  data: 0.0001  max mem: 23580
Train Epoch: [18] Total time: 0:03:01 (0.6352 s / it)
Averaged stats: lr: 0.0000  loss_ita: 2.9065  loss_itm: 0.2066  loss_lm: 3.5795
Train Epoch: [19]  [  0/285]  eta: 0:06:13  lr: 0.000004  loss_ita: 3.8829  loss_itm: 0.2680  loss_lm: 3.3353  time: 1.3106  data: 0.4987  max mem: 23580
Train Epoch: [19]  [ 50/285]  eta: 0:02:31  lr: 0.000004  loss_ita: 2.5315  loss_itm: 0.0676  loss_lm: 3.8670  time: 0.6306  data: 0.0001  max mem: 23581
Train Epoch: [19]  [100/285]  eta: 0:01:58  lr: 0.000004  loss_ita: 2.7679  loss_itm: 0.1157  loss_lm: 3.5858  time: 0.6325  data: 0.0001  max mem: 23581
Train Epoch: [19]  [150/285]  eta: 0:01:25  lr: 0.000004  loss_ita: 2.6944  loss_itm: 0.1165  loss_lm: 3.8282  time: 0.6296  data: 0.0001  max mem: 23581
Train Epoch: [19]  [200/285]  eta: 0:00:53  lr: 0.000004  loss_ita: 2.7883  loss_itm: 0.3842  loss_lm: 3.5960  time: 0.6310  data: 0.0001  max mem: 23581
Train Epoch: [19]  [250/285]  eta: 0:00:22  lr: 0.000004  loss_ita: 2.8382  loss_itm: 0.0739  loss_lm: 3.8309  time: 0.6290  data: 0.0001  max mem: 23583
Train Epoch: [19]  [284/285]  eta: 0:00:00  lr: 0.000004  loss_ita: 2.0684  loss_itm: 0.0190  loss_lm: 3.4953  time: 0.6300  data: 0.0001  max mem: 23583
Train Epoch: [19] Total time: 0:03:00 (0.6337 s / it)
Averaged stats: lr: 0.0000  loss_ita: 3.0496  loss_itm: 0.1981  loss_lm: 3.5696
Train Epoch: [20]  [  0/285]  eta: 0:06:16  lr: 0.000004  loss_ita: 3.9326  loss_itm: 0.2766  loss_lm: 3.5922  time: 1.3222  data: 0.5076  max mem: 23583
Train Epoch: [20]  [ 50/285]  eta: 0:02:31  lr: 0.000004  loss_ita: 2.4996  loss_itm: 0.0818  loss_lm: 3.3318  time: 0.6295  data: 0.0001  max mem: 23587
Train Epoch: [20]  [100/285]  eta: 0:01:57  lr: 0.000004  loss_ita: 2.6665  loss_itm: 0.1794  loss_lm: 3.6086  time: 0.6303  data: 0.0001  max mem: 23587
Train Epoch: [20]  [150/285]  eta: 0:01:25  lr: 0.000004  loss_ita: 2.7753  loss_itm: 0.0685  loss_lm: 3.9440  time: 0.6295  data: 0.0001  max mem: 23587
Train Epoch: [20]  [200/285]  eta: 0:00:53  lr: 0.000004  loss_ita: 3.2826  loss_itm: 0.2156  loss_lm: 3.6331  time: 0.6314  data: 0.0001  max mem: 23588
Train Epoch: [20]  [250/285]  eta: 0:00:22  lr: 0.000004  loss_ita: 2.6591  loss_itm: 0.0412  loss_lm: 3.5451  time: 0.6378  data: 0.0001  max mem: 23588
Train Epoch: [20]  [284/285]  eta: 0:00:00  lr: 0.000004  loss_ita: 2.6827  loss_itm: 0.1384  loss_lm: 3.6881  time: 0.6287  data: 0.0001  max mem: 23588
Train Epoch: [20] Total time: 0:03:00 (0.6343 s / it)
Averaged stats: lr: 0.0000  loss_ita: 3.0540  loss_itm: 0.1896  loss_lm: 3.5592
Train Epoch: [21]  [  0/285]  eta: 0:06:11  lr: 0.000003  loss_ita: 2.0090  loss_itm: 0.1198  loss_lm: 3.8778  time: 1.3040  data: 0.5166  max mem: 23588
Train Epoch: [21]  [ 50/285]  eta: 0:02:31  lr: 0.000003  loss_ita: 3.2114  loss_itm: 0.0530  loss_lm: 3.4357  time: 0.6324  data: 0.0001  max mem: 23588
Train Epoch: [21]  [100/285]  eta: 0:01:58  lr: 0.000003  loss_ita: 2.7815  loss_itm: 0.0497  loss_lm: 3.3207  time: 0.6313  data: 0.0001  max mem: 23588
Train Epoch: [21]  [150/285]  eta: 0:01:25  lr: 0.000003  loss_ita: 2.9811  loss_itm: 0.0588  loss_lm: 3.4174  time: 0.6283  data: 0.0001  max mem: 23588
Train Epoch: [21]  [200/285]  eta: 0:00:53  lr: 0.000003  loss_ita: 1.9606  loss_itm: 0.0440  loss_lm: 3.5696  time: 0.6297  data: 0.0001  max mem: 23588
Train Epoch: [21]  [250/285]  eta: 0:00:22  lr: 0.000003  loss_ita: 2.6679  loss_itm: 0.0967  loss_lm: 3.4538  time: 0.6298  data: 0.0001  max mem: 23590
Train Epoch: [21]  [284/285]  eta: 0:00:00  lr: 0.000003  loss_ita: 3.9391  loss_itm: 0.2248  loss_lm: 3.9160  time: 0.6294  data: 0.0001  max mem: 23590
Train Epoch: [21] Total time: 0:03:00 (0.6334 s / it)
Averaged stats: lr: 0.0000  loss_ita: 2.9200  loss_itm: 0.1851  loss_lm: 3.5467
Train Epoch: [22]  [  0/285]  eta: 0:06:15  lr: 0.000003  loss_ita: 3.8348  loss_itm: 0.1921  loss_lm: 3.5681  time: 1.3165  data: 0.5270  max mem: 23590
Train Epoch: [22]  [ 50/285]  eta: 0:02:31  lr: 0.000003  loss_ita: 3.4569  loss_itm: 0.1682  loss_lm: 3.4865  time: 0.6355  data: 0.0001  max mem: 23591
Train Epoch: [22]  [100/285]  eta: 0:01:58  lr: 0.000003  loss_ita: 4.0777  loss_itm: 0.1910  loss_lm: 3.1981  time: 0.6303  data: 0.0001  max mem: 23591
Train Epoch: [22]  [150/285]  eta: 0:01:25  lr: 0.000003  loss_ita: 2.6585  loss_itm: 0.0517  loss_lm: 3.6514  time: 0.6320  data: 0.0001  max mem: 23591
Train Epoch: [22]  [200/285]  eta: 0:00:53  lr: 0.000003  loss_ita: 3.6554  loss_itm: 0.4904  loss_lm: 3.4316  time: 0.6315  data: 0.0001  max mem: 23591
Train Epoch: [22]  [250/285]  eta: 0:00:22  lr: 0.000003  loss_ita: 3.1082  loss_itm: 0.0870  loss_lm: 3.1441  time: 0.6306  data: 0.0001  max mem: 23591
Train Epoch: [22]  [284/285]  eta: 0:00:00  lr: 0.000003  loss_ita: 2.3675  loss_itm: 0.1813  loss_lm: 3.3992  time: 0.6389  data: 0.0001  max mem: 23591
Train Epoch: [22] Total time: 0:03:00 (0.6347 s / it)
Averaged stats: lr: 0.0000  loss_ita: 2.9872  loss_itm: 0.1731  loss_lm: 3.5371
Train Epoch: [23]  [  0/285]  eta: 0:06:09  lr: 0.000003  loss_ita: 3.3857  loss_itm: 0.1952  loss_lm: 3.4450  time: 1.2952  data: 0.5210  max mem: 23591
Train Epoch: [23]  [ 50/285]  eta: 0:02:31  lr: 0.000003  loss_ita: 2.8356  loss_itm: 0.1286  loss_lm: 3.0219  time: 0.6310  data: 0.0001  max mem: 23591
Train Epoch: [23]  [100/285]  eta: 0:01:58  lr: 0.000003  loss_ita: 3.1510  loss_itm: 0.1504  loss_lm: 3.3765  time: 0.6314  data: 0.0001  max mem: 23591
Train Epoch: [23]  [150/285]  eta: 0:01:25  lr: 0.000003  loss_ita: 3.3297  loss_itm: 0.2624  loss_lm: 3.5163  time: 0.6306  data: 0.0001  max mem: 23591
Train Epoch: [23]  [200/285]  eta: 0:00:53  lr: 0.000003  loss_ita: 2.1571  loss_itm: 0.0379  loss_lm: 3.9258  time: 0.6325  data: 0.0001  max mem: 23591
Train Epoch: [23]  [250/285]  eta: 0:00:22  lr: 0.000003  loss_ita: 2.6338  loss_itm: 0.1062  loss_lm: 3.7426  time: 0.6297  data: 0.0001  max mem: 23591
Train Epoch: [23]  [284/285]  eta: 0:00:00  lr: 0.000003  loss_ita: 2.5628  loss_itm: 0.2568  loss_lm: 3.5695  time: 0.6310  data: 0.0001  max mem: 23591
Train Epoch: [23] Total time: 0:03:00 (0.6344 s / it)
Averaged stats: lr: 0.0000  loss_ita: 2.8562  loss_itm: 0.1716  loss_lm: 3.5298
Train Epoch: [24]  [  0/285]  eta: 0:06:09  lr: 0.000002  loss_ita: 3.0546  loss_itm: 0.2612  loss_lm: 3.0972  time: 1.2955  data: 0.4992  max mem: 23591
Train Epoch: [24]  [ 50/285]  eta: 0:02:31  lr: 0.000002  loss_ita: 2.5016  loss_itm: 0.0623  loss_lm: 3.8880  time: 0.6316  data: 0.0001  max mem: 23591
Train Epoch: [24]  [100/285]  eta: 0:01:57  lr: 0.000002  loss_ita: 2.5044  loss_itm: 0.0338  loss_lm: 3.3143  time: 0.6297  data: 0.0001  max mem: 23591
Train Epoch: [24]  [150/285]  eta: 0:01:25  lr: 0.000002  loss_ita: 3.8728  loss_itm: 0.2768  loss_lm: 3.0960  time: 0.6298  data: 0.0001  max mem: 23591
Train Epoch: [24]  [200/285]  eta: 0:00:53  lr: 0.000002  loss_ita: 2.7933  loss_itm: 0.1601  loss_lm: 3.5249  time: 0.6300  data: 0.0001  max mem: 23591
Train Epoch: [24]  [250/285]  eta: 0:00:22  lr: 0.000002  loss_ita: 2.8936  loss_itm: 0.1368  loss_lm: 3.3508  time: 0.6321  data: 0.0001  max mem: 23591
Train Epoch: [24]  [284/285]  eta: 0:00:00  lr: 0.000002  loss_ita: 2.6732  loss_itm: 0.0911  loss_lm: 3.5067  time: 0.6302  data: 0.0001  max mem: 23591
Train Epoch: [24] Total time: 0:03:00 (0.6339 s / it)
Averaged stats: lr: 0.0000  loss_ita: 2.7934  loss_itm: 0.1733  loss_lm: 3.5243
Train Epoch: [25]  [  0/285]  eta: 0:06:09  lr: 0.000002  loss_ita: 2.4046  loss_itm: 0.0646  loss_lm: 3.1212  time: 1.2975  data: 0.5013  max mem: 23591
Train Epoch: [25]  [ 50/285]  eta: 0:02:32  lr: 0.000002  loss_ita: 2.4101  loss_itm: 0.0487  loss_lm: 3.6172  time: 0.6299  data: 0.0001  max mem: 23591
Train Epoch: [25]  [100/285]  eta: 0:01:58  lr: 0.000002  loss_ita: 2.6374  loss_itm: 0.2982  loss_lm: 3.5690  time: 0.6329  data: 0.0001  max mem: 23591
Train Epoch: [25]  [150/285]  eta: 0:01:25  lr: 0.000002  loss_ita: 2.7885  loss_itm: 0.1721  loss_lm: 3.6028  time: 0.6306  data: 0.0001  max mem: 23591
Train Epoch: [25]  [200/285]  eta: 0:00:53  lr: 0.000002  loss_ita: 2.7185  loss_itm: 0.3585  loss_lm: 3.3039  time: 0.6306  data: 0.0001  max mem: 23591
Train Epoch: [25]  [250/285]  eta: 0:00:22  lr: 0.000002  loss_ita: 3.7675  loss_itm: 0.2942  loss_lm: 3.5000  time: 0.6295  data: 0.0001  max mem: 23591
Train Epoch: [25]  [284/285]  eta: 0:00:00  lr: 0.000002  loss_ita: 2.9141  loss_itm: 0.2211  loss_lm: 3.7471  time: 0.6303  data: 0.0001  max mem: 23591
Train Epoch: [25] Total time: 0:03:00 (0.6345 s / it)
Averaged stats: lr: 0.0000  loss_ita: 2.7133  loss_itm: 0.1624  loss_lm: 3.5156
Train Epoch: [26]  [  0/285]  eta: 0:06:17  lr: 0.000002  loss_ita: 3.2285  loss_itm: 0.2084  loss_lm: 3.3316  time: 1.3244  data: 0.5321  max mem: 23591
Train Epoch: [26]  [ 50/285]  eta: 0:02:32  lr: 0.000002  loss_ita: 3.1342  loss_itm: 0.1387  loss_lm: 3.2193  time: 0.6316  data: 0.0001  max mem: 23591
Train Epoch: [26]  [100/285]  eta: 0:01:58  lr: 0.000002  loss_ita: 2.6800  loss_itm: 0.0607  loss_lm: 3.4500  time: 0.6315  data: 0.0001  max mem: 23591
Train Epoch: [26]  [150/285]  eta: 0:01:25  lr: 0.000002  loss_ita: 1.9819  loss_itm: 0.0415  loss_lm: 3.8939  time: 0.6322  data: 0.0001  max mem: 23591
Train Epoch: [26]  [200/285]  eta: 0:00:54  lr: 0.000002  loss_ita: 2.5661  loss_itm: 0.0929  loss_lm: 3.0562  time: 0.6310  data: 0.0001  max mem: 23591
Train Epoch: [26]  [250/285]  eta: 0:00:22  lr: 0.000002  loss_ita: 2.8299  loss_itm: 0.1418  loss_lm: 3.8424  time: 0.6311  data: 0.0001  max mem: 23591
Train Epoch: [26]  [284/285]  eta: 0:00:00  lr: 0.000002  loss_ita: 2.2377  loss_itm: 0.0293  loss_lm: 3.4249  time: 0.6307  data: 0.0001  max mem: 23591
Train Epoch: [26] Total time: 0:03:00 (0.6351 s / it)
Averaged stats: lr: 0.0000  loss_ita: 2.6755  loss_itm: 0.1594  loss_lm: 3.5116
Train Epoch: [27]  [  0/285]  eta: 0:06:14  lr: 0.000002  loss_ita: 1.9781  loss_itm: 0.0223  loss_lm: 3.4342  time: 1.3134  data: 0.5306  max mem: 23591
Train Epoch: [27]  [ 50/285]  eta: 0:02:32  lr: 0.000002  loss_ita: 2.9793  loss_itm: 0.2779  loss_lm: 3.2218  time: 0.6358  data: 0.0001  max mem: 23591
Train Epoch: [27]  [100/285]  eta: 0:01:58  lr: 0.000002  loss_ita: 3.9257  loss_itm: 0.3200  loss_lm: 3.6128  time: 0.6322  data: 0.0001  max mem: 23591
Train Epoch: [27]  [150/285]  eta: 0:01:26  lr: 0.000002  loss_ita: 2.1720  loss_itm: 0.0347  loss_lm: 3.4662  time: 0.6321  data: 0.0001  max mem: 23591
Train Epoch: [27]  [200/285]  eta: 0:00:54  lr: 0.000002  loss_ita: 1.7170  loss_itm: 0.0166  loss_lm: 3.4224  time: 0.6312  data: 0.0001  max mem: 23591
Train Epoch: [27]  [250/285]  eta: 0:00:22  lr: 0.000002  loss_ita: 3.1334  loss_itm: 0.1594  loss_lm: 3.2232  time: 0.6307  data: 0.0001  max mem: 23591
Train Epoch: [27]  [284/285]  eta: 0:00:00  lr: 0.000002  loss_ita: 3.7465  loss_itm: 0.1747  loss_lm: 3.3571  time: 0.6297  data: 0.0001  max mem: 23591
Train Epoch: [27] Total time: 0:03:00 (0.6349 s / it)
Averaged stats: lr: 0.0000  loss_ita: 2.5996  loss_itm: 0.1592  loss_lm: 3.5072
Train Epoch: [28]  [  0/285]  eta: 0:06:04  lr: 0.000002  loss_ita: 3.0197  loss_itm: 0.1638  loss_lm: 3.2848  time: 1.2801  data: 0.5232  max mem: 23591
Train Epoch: [28]  [ 50/285]  eta: 0:02:32  lr: 0.000002  loss_ita: 2.2830  loss_itm: 0.3292  loss_lm: 3.5122  time: 0.6359  data: 0.0001  max mem: 23591
Train Epoch: [28]  [100/285]  eta: 0:01:58  lr: 0.000002  loss_ita: 2.7211  loss_itm: 0.1661  loss_lm: 3.0131  time: 0.6333  data: 0.0001  max mem: 23591
Train Epoch: [28]  [150/285]  eta: 0:01:26  lr: 0.000002  loss_ita: 2.6901  loss_itm: 0.0696  loss_lm: 3.3174  time: 0.6314  data: 0.0001  max mem: 23591
Train Epoch: [28]  [200/285]  eta: 0:00:54  lr: 0.000002  loss_ita: 3.6141  loss_itm: 0.2602  loss_lm: 3.3708  time: 0.6301  data: 0.0001  max mem: 23591
Train Epoch: [28]  [250/285]  eta: 0:00:22  lr: 0.000002  loss_ita: 2.4555  loss_itm: 0.2227  loss_lm: 3.5199  time: 0.6302  data: 0.0001  max mem: 23591
Train Epoch: [28]  [284/285]  eta: 0:00:00  lr: 0.000002  loss_ita: 2.1547  loss_itm: 0.1005  loss_lm: 3.4906  time: 0.6307  data: 0.0001  max mem: 23591
Train Epoch: [28] Total time: 0:03:01 (0.6352 s / it)
Averaged stats: lr: 0.0000  loss_ita: 2.5119  loss_itm: 0.1590  loss_lm: 3.5039
Train Epoch: [29]  [  0/285]  eta: 0:06:09  lr: 0.000001  loss_ita: 2.6196  loss_itm: 0.0961  loss_lm: 3.5687  time: 1.2970  data: 0.5059  max mem: 23591
Train Epoch: [29]  [ 50/285]  eta: 0:02:31  lr: 0.000001  loss_ita: 2.6856  loss_itm: 0.0960  loss_lm: 3.3383  time: 0.6322  data: 0.0001  max mem: 23591
Train Epoch: [29]  [100/285]  eta: 0:01:58  lr: 0.000001  loss_ita: 3.3120  loss_itm: 0.1161  loss_lm: 3.7155  time: 0.6298  data: 0.0001  max mem: 23591
Train Epoch: [29]  [150/285]  eta: 0:01:26  lr: 0.000001  loss_ita: 3.0453  loss_itm: 0.3258  loss_lm: 3.1343  time: 0.6319  data: 0.0001  max mem: 23591
Train Epoch: [29]  [200/285]  eta: 0:00:54  lr: 0.000001  loss_ita: 1.7908  loss_itm: 0.0233  loss_lm: 3.4499  time: 0.6316  data: 0.0001  max mem: 23591
Train Epoch: [29]  [250/285]  eta: 0:00:22  lr: 0.000001  loss_ita: 2.6055  loss_itm: 0.0955  loss_lm: 3.4520  time: 0.6322  data: 0.0001  max mem: 23591
Train Epoch: [29]  [284/285]  eta: 0:00:00  lr: 0.000001  loss_ita: 2.7479  loss_itm: 0.2525  loss_lm: 3.3004  time: 0.6300  data: 0.0001  max mem: 23591
Train Epoch: [29] Total time: 0:03:01 (0.6352 s / it)
Averaged stats: lr: 0.0000  loss_ita: 2.5454  loss_itm: 0.1528  loss_lm: 3.5015
Train Epoch: [30]  [  0/285]  eta: 0:06:10  lr: 0.000001  loss_ita: 2.4197  loss_itm: 0.1027  loss_lm: 3.7645  time: 1.3002  data: 0.4899  max mem: 23591
Train Epoch: [30]  [ 50/285]  eta: 0:02:31  lr: 0.000001  loss_ita: 3.1213  loss_itm: 0.2610  loss_lm: 3.3778  time: 0.6292  data: 0.0001  max mem: 23591
Train Epoch: [30]  [100/285]  eta: 0:01:57  lr: 0.000001  loss_ita: 2.6514  loss_itm: 0.1126  loss_lm: 3.3477  time: 0.6305  data: 0.0001  max mem: 23591
Train Epoch: [30]  [150/285]  eta: 0:01:25  lr: 0.000001  loss_ita: 2.4420  loss_itm: 0.1276  loss_lm: 3.6337  time: 0.6309  data: 0.0001  max mem: 23591
Train Epoch: [30]  [200/285]  eta: 0:00:53  lr: 0.000001  loss_ita: 3.0520  loss_itm: 0.7821  loss_lm: 3.5655  time: 0.6301  data: 0.0001  max mem: 23591
Train Epoch: [30]  [250/285]  eta: 0:00:22  lr: 0.000001  loss_ita: 2.3376  loss_itm: 0.0408  loss_lm: 3.8197  time: 0.6301  data: 0.0001  max mem: 23591
Train Epoch: [30]  [284/285]  eta: 0:00:00  lr: 0.000001  loss_ita: 2.9708  loss_itm: 0.1513  loss_lm: 3.4485  time: 0.6285  data: 0.0001  max mem: 23591
Train Epoch: [30] Total time: 0:03:00 (0.6334 s / it)
Averaged stats: lr: 0.0000  loss_ita: 2.6407  loss_itm: 0.1503  loss_lm: 3.4926
Train Epoch: [31]  [  0/285]  eta: 0:06:11  lr: 0.000001  loss_ita: 2.8827  loss_itm: 0.4515  loss_lm: 3.6794  time: 1.3045  data: 0.5192  max mem: 23591
Train Epoch: [31]  [ 50/285]  eta: 0:02:31  lr: 0.000001  loss_ita: 2.7039  loss_itm: 0.2288  loss_lm: 3.4772  time: 0.6314  data: 0.0001  max mem: 23591
Train Epoch: [31]  [100/285]  eta: 0:01:58  lr: 0.000001  loss_ita: 3.4030  loss_itm: 0.2102  loss_lm: 3.2048  time: 0.6377  data: 0.0001  max mem: 23591
Train Epoch: [31]  [150/285]  eta: 0:01:25  lr: 0.000001  loss_ita: 2.1047  loss_itm: 0.1790  loss_lm: 3.2767  time: 0.6302  data: 0.0001  max mem: 23591
Train Epoch: [31]  [200/285]  eta: 0:00:53  lr: 0.000001  loss_ita: 2.9920  loss_itm: 0.1207  loss_lm: 3.2872  time: 0.6301  data: 0.0001  max mem: 23591
Train Epoch: [31]  [250/285]  eta: 0:00:22  lr: 0.000001  loss_ita: 2.0734  loss_itm: 0.0986  loss_lm: 3.3521  time: 0.6306  data: 0.0001  max mem: 23591
Train Epoch: [31]  [284/285]  eta: 0:00:00  lr: 0.000001  loss_ita: 2.5143  loss_itm: 0.1509  loss_lm: 3.0104  time: 0.6311  data: 0.0001  max mem: 23591
Train Epoch: [31] Total time: 0:03:00 (0.6349 s / it)
Averaged stats: lr: 0.0000  loss_ita: 2.6926  loss_itm: 0.1486  loss_lm: 3.4901
Train Epoch: [32]  [  0/285]  eta: 0:06:08  lr: 0.000001  loss_ita: 3.2772  loss_itm: 0.1949  loss_lm: 3.4501  time: 1.2939  data: 0.5200  max mem: 23591
Train Epoch: [32]  [ 50/285]  eta: 0:02:31  lr: 0.000001  loss_ita: 3.1804  loss_itm: 0.2954  loss_lm: 3.4395  time: 0.6308  data: 0.0001  max mem: 23591
Train Epoch: [32]  [100/285]  eta: 0:01:57  lr: 0.000001  loss_ita: 2.9167  loss_itm: 0.0901  loss_lm: 3.2657  time: 0.6313  data: 0.0001  max mem: 23591
Train Epoch: [32]  [150/285]  eta: 0:01:25  lr: 0.000001  loss_ita: 3.2093  loss_itm: 0.2000  loss_lm: 3.4597  time: 0.6303  data: 0.0001  max mem: 23591
Train Epoch: [32]  [200/285]  eta: 0:00:53  lr: 0.000001  loss_ita: 3.1401  loss_itm: 0.1410  loss_lm: 3.1709  time: 0.6300  data: 0.0001  max mem: 23591
Train Epoch: [32]  [250/285]  eta: 0:00:22  lr: 0.000001  loss_ita: 3.1026  loss_itm: 0.2538  loss_lm: 3.3451  time: 0.6287  data: 0.0001  max mem: 23591
Train Epoch: [32]  [284/285]  eta: 0:00:00  lr: 0.000001  loss_ita: 2.4895  loss_itm: 0.0710  loss_lm: 3.2660  time: 0.6304  data: 0.0001  max mem: 23591
Train Epoch: [32] Total time: 0:03:00 (0.6335 s / it)
Averaged stats: lr: 0.0000  loss_ita: 2.6679  loss_itm: 0.1489  loss_lm: 3.4901
Train Epoch: [33]  [  0/285]  eta: 0:06:01  lr: 0.000001  loss_ita: 2.7554  loss_itm: 0.2933  loss_lm: 3.6843  time: 1.2675  data: 0.5124  max mem: 23591
Train Epoch: [33]  [ 50/285]  eta: 0:02:30  lr: 0.000001  loss_ita: 2.2648  loss_itm: 0.0123  loss_lm: 3.8396  time: 0.6301  data: 0.0001  max mem: 23591
Train Epoch: [33]  [100/285]  eta: 0:01:57  lr: 0.000001  loss_ita: 3.4152  loss_itm: 0.1820  loss_lm: 3.0143  time: 0.6332  data: 0.0001  max mem: 23592
Train Epoch: [33]  [150/285]  eta: 0:01:25  lr: 0.000001  loss_ita: 2.8898  loss_itm: 0.1346  loss_lm: 3.6108  time: 0.6339  data: 0.0001  max mem: 23592
Train Epoch: [33]  [200/285]  eta: 0:00:54  lr: 0.000001  loss_ita: 1.7518  loss_itm: 0.0852  loss_lm: 3.3655  time: 0.6311  data: 0.0001  max mem: 23592
Train Epoch: [33]  [250/285]  eta: 0:00:22  lr: 0.000001  loss_ita: 3.0699  loss_itm: 0.1652  loss_lm: 3.1280  time: 0.6326  data: 0.0001  max mem: 23592
Train Epoch: [33]  [284/285]  eta: 0:00:00  lr: 0.000001  loss_ita: 3.5743  loss_itm: 0.2519  loss_lm: 3.3620  time: 0.6304  data: 0.0001  max mem: 23592
Train Epoch: [33] Total time: 0:03:00 (0.6350 s / it)
Averaged stats: lr: 0.0000  loss_ita: 2.5888  loss_itm: 0.1543  loss_lm: 3.4868
Train Epoch: [34]  [  0/285]  eta: 0:06:15  lr: 0.000001  loss_ita: 2.3373  loss_itm: 0.3223  loss_lm: 3.6136  time: 1.3172  data: 0.5307  max mem: 23592
Train Epoch: [34]  [ 50/285]  eta: 0:02:31  lr: 0.000001  loss_ita: 2.7148  loss_itm: 0.2511  loss_lm: 3.1772  time: 0.6325  data: 0.0001  max mem: 23592
Train Epoch: [34]  [100/285]  eta: 0:01:58  lr: 0.000001  loss_ita: 2.9896  loss_itm: 0.1923  loss_lm: 3.9364  time: 0.6358  data: 0.0001  max mem: 23592
Train Epoch: [34]  [150/285]  eta: 0:01:26  lr: 0.000001  loss_ita: 2.6831  loss_itm: 0.1087  loss_lm: 3.6951  time: 0.6330  data: 0.0001  max mem: 23592
Train Epoch: [34]  [200/285]  eta: 0:00:54  lr: 0.000001  loss_ita: 2.7463  loss_itm: 0.1221  loss_lm: 3.6522  time: 0.6316  data: 0.0001  max mem: 23592
Train Epoch: [34]  [250/285]  eta: 0:00:22  lr: 0.000001  loss_ita: 3.0686  loss_itm: 0.2842  loss_lm: 3.5451  time: 0.6312  data: 0.0001  max mem: 23592
Train Epoch: [34]  [284/285]  eta: 0:00:00  lr: 0.000001  loss_ita: 2.0698  loss_itm: 0.0205  loss_lm: 3.3607  time: 0.6311  data: 0.0001  max mem: 23592
Train Epoch: [34] Total time: 0:03:01 (0.6360 s / it)
Averaged stats: lr: 0.0000  loss_ita: 2.5445  loss_itm: 0.1429  loss_lm: 3.4853
Traceback (most recent call last):
  File "/home/monajati/miniconda3/envs/blip/lib/python3.7/site-packages/torch/serialization.py", line 379, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol)
  File "/home/monajati/miniconda3/envs/blip/lib/python3.7/site-packages/torch/serialization.py", line 499, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
OSError: [Errno 28] No space left on device

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "pretrain.py", line 176, in <module>
    main(args, config)
  File "pretrain.py", line 145, in main
    torch.save(save_obj, os.path.join(args.output_dir, 'checkpoint_%02d.pth'%epoch))  
  File "/home/monajati/miniconda3/envs/blip/lib/python3.7/site-packages/torch/serialization.py", line 380, in save
    return
  File "/home/monajati/miniconda3/envs/blip/lib/python3.7/site-packages/torch/serialization.py", line 259, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:300] . unexpected pos 2689633024 vs 2689632912
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2078114 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2078116 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2078119 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -6) local_rank: 0 (pid: 2078112) of binary: /home/monajati/miniconda3/envs/blip/bin/python
Traceback (most recent call last):
  File "/home/monajati/miniconda3/envs/blip/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/monajati/miniconda3/envs/blip/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/monajati/miniconda3/envs/blip/lib/python3.7/site-packages/torch/distributed/run.py", line 723, in <module>
    main()
  File "/home/monajati/miniconda3/envs/blip/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/monajati/miniconda3/envs/blip/lib/python3.7/site-packages/torch/distributed/run.py", line 719, in main
    run(args)
  File "/home/monajati/miniconda3/envs/blip/lib/python3.7/site-packages/torch/distributed/run.py", line 713, in run
    )(*cmd_args)
  File "/home/monajati/miniconda3/envs/blip/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/monajati/miniconda3/envs/blip/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 261, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
pretrain.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-10-05_00:33:36
  host      : pluslab-a6000
  rank      : 0 (local_rank: 0)
  exitcode  : -6 (pid: 2078112)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 2078112
========================================================
