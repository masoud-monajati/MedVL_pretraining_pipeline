nohup: ignoring input
| distributed init (rank 0, word 1): env://
Creating dataset
loading /home/monajati/main/med/blip/med_data/med_captions_processed.json
number of training samples: 39535
Creating model
/embeddings/word_embeddings is tied
/embeddings/position_embeddings is tied
/embeddings/LayerNorm is tied
/encoder/layer/0/crossattention/self/query is tied
/encoder/layer/0/crossattention/self/key is tied
/encoder/layer/0/crossattention/self/value is tied
/encoder/layer/0/crossattention/output/dense is tied
/encoder/layer/0/crossattention/output/LayerNorm is tied
/encoder/layer/0/intermediate/dense is tied
/encoder/layer/0/output/dense is tied
/encoder/layer/0/output/LayerNorm is tied
/encoder/layer/1/crossattention/self/query is tied
/encoder/layer/1/crossattention/self/key is tied
/encoder/layer/1/crossattention/self/value is tied
/encoder/layer/1/crossattention/output/dense is tied
/encoder/layer/1/crossattention/output/LayerNorm is tied
/encoder/layer/1/intermediate/dense is tied
/encoder/layer/1/output/dense is tied
/encoder/layer/1/output/LayerNorm is tied
/encoder/layer/2/crossattention/self/query is tied
/encoder/layer/2/crossattention/self/key is tied
/encoder/layer/2/crossattention/self/value is tied
/encoder/layer/2/crossattention/output/dense is tied
/encoder/layer/2/crossattention/output/LayerNorm is tied
/encoder/layer/2/intermediate/dense is tied
/encoder/layer/2/output/dense is tied
/encoder/layer/2/output/LayerNorm is tied
/encoder/layer/3/crossattention/self/query is tied
/encoder/layer/3/crossattention/self/key is tied
/encoder/layer/3/crossattention/self/value is tied
/encoder/layer/3/crossattention/output/dense is tied
/encoder/layer/3/crossattention/output/LayerNorm is tied
/encoder/layer/3/intermediate/dense is tied
/encoder/layer/3/output/dense is tied
/encoder/layer/3/output/LayerNorm is tied
/encoder/layer/4/crossattention/self/query is tied
/encoder/layer/4/crossattention/self/key is tied
/encoder/layer/4/crossattention/self/value is tied
/encoder/layer/4/crossattention/output/dense is tied
/encoder/layer/4/crossattention/output/LayerNorm is tied
/encoder/layer/4/intermediate/dense is tied
/encoder/layer/4/output/dense is tied
/encoder/layer/4/output/LayerNorm is tied
/encoder/layer/5/crossattention/self/query is tied
/encoder/layer/5/crossattention/self/key is tied
/encoder/layer/5/crossattention/self/value is tied
/encoder/layer/5/crossattention/output/dense is tied
/encoder/layer/5/crossattention/output/LayerNorm is tied
/encoder/layer/5/intermediate/dense is tied
/encoder/layer/5/output/dense is tied
/encoder/layer/5/output/LayerNorm is tied
/encoder/layer/6/crossattention/self/query is tied
/encoder/layer/6/crossattention/self/key is tied
/encoder/layer/6/crossattention/self/value is tied
/encoder/layer/6/crossattention/output/dense is tied
/encoder/layer/6/crossattention/output/LayerNorm is tied
/encoder/layer/6/intermediate/dense is tied
/encoder/layer/6/output/dense is tied
/encoder/layer/6/output/LayerNorm is tied
/encoder/layer/7/crossattention/self/query is tied
/encoder/layer/7/crossattention/self/key is tied
/encoder/layer/7/crossattention/self/value is tied
/encoder/layer/7/crossattention/output/dense is tied
/encoder/layer/7/crossattention/output/LayerNorm is tied
/encoder/layer/7/intermediate/dense is tied
/encoder/layer/7/output/dense is tied
/encoder/layer/7/output/LayerNorm is tied
/encoder/layer/8/crossattention/self/query is tied
/encoder/layer/8/crossattention/self/key is tied
/encoder/layer/8/crossattention/self/value is tied
/encoder/layer/8/crossattention/output/dense is tied
/encoder/layer/8/crossattention/output/LayerNorm is tied
/encoder/layer/8/intermediate/dense is tied
/encoder/layer/8/output/dense is tied
/encoder/layer/8/output/LayerNorm is tied
/encoder/layer/9/crossattention/self/query is tied
/encoder/layer/9/crossattention/self/key is tied
/encoder/layer/9/crossattention/self/value is tied
/encoder/layer/9/crossattention/output/dense is tied
/encoder/layer/9/crossattention/output/LayerNorm is tied
/encoder/layer/9/intermediate/dense is tied
/encoder/layer/9/output/dense is tied
/encoder/layer/9/output/LayerNorm is tied
/encoder/layer/10/crossattention/self/query is tied
/encoder/layer/10/crossattention/self/key is tied
/encoder/layer/10/crossattention/self/value is tied
/encoder/layer/10/crossattention/output/dense is tied
/encoder/layer/10/crossattention/output/LayerNorm is tied
/encoder/layer/10/intermediate/dense is tied
/encoder/layer/10/output/dense is tied
/encoder/layer/10/output/LayerNorm is tied
/encoder/layer/11/crossattention/self/query is tied
/encoder/layer/11/crossattention/self/key is tied
/encoder/layer/11/crossattention/self/value is tied
/encoder/layer/11/crossattention/output/dense is tied
/encoder/layer/11/crossattention/output/LayerNorm is tied
/encoder/layer/11/intermediate/dense is tied
/encoder/layer/11/output/dense is tied
/encoder/layer/11/output/LayerNorm is tied
resume checkpoint from /home/monajati/main/med/blip/model_base.pth
args.gpu 0
Start training
Train Epoch: [0]  [   0/3953]  eta: 17:36:54  lr: 0.000001  loss_ita: 4.7094  loss_itm: 2.0780  loss_lm: 8.2256  time: 16.0421  data: 10.6387  max mem: 17311
Train Epoch: [0]  [  50/3953]  eta: 2:17:13  lr: 0.000006  loss_ita: 4.9521  loss_itm: 0.6221  loss_lm: 6.5999  time: 1.7776  data: 1.0016  max mem: 20286
Train Epoch: [0]  [ 100/3953]  eta: 2:20:56  lr: 0.000011  loss_ita: 4.3376  loss_itm: 0.7432  loss_lm: 6.5458  time: 2.0667  data: 1.2914  max mem: 20294
Train Epoch: [0]  [ 150/3953]  eta: 2:10:29  lr: 0.000015  loss_ita: 3.7977  loss_itm: 0.5985  loss_lm: 5.8878  time: 1.9795  data: 1.2022  max mem: 20294
Train Epoch: [0]  [ 200/3953]  eta: 2:05:45  lr: 0.000020  loss_ita: 3.8911  loss_itm: 0.6633  loss_lm: 5.6829  time: 1.6507  data: 0.8722  max mem: 20294
Train Epoch: [0]  [ 250/3953]  eta: 2:01:39  lr: 0.000025  loss_ita: 3.4716  loss_itm: 0.6829  loss_lm: 5.9860  time: 1.7978  data: 1.0226  max mem: 20296
Train Epoch: [0]  [ 300/3953]  eta: 2:00:33  lr: 0.000030  loss_ita: 3.0759  loss_itm: 0.6468  loss_lm: 5.5812  time: 2.0931  data: 1.3170  max mem: 20296
Train Epoch: [0]  [ 350/3953]  eta: 1:59:18  lr: 0.000030  loss_ita: 3.2888  loss_itm: 0.5920  loss_lm: 5.4503  time: 2.2232  data: 1.4473  max mem: 20296
Train Epoch: [0]  [ 400/3953]  eta: 1:58:44  lr: 0.000030  loss_ita: 3.2621  loss_itm: 0.4893  loss_lm: 5.4548  time: 2.1940  data: 1.4206  max mem: 20296
Train Epoch: [0]  [ 450/3953]  eta: 1:57:07  lr: 0.000030  loss_ita: 3.6910  loss_itm: 0.5504  loss_lm: 5.4981  time: 1.9713  data: 1.1965  max mem: 20296
Train Epoch: [0]  [ 500/3953]  eta: 1:55:37  lr: 0.000030  loss_ita: 2.9578  loss_itm: 0.5856  loss_lm: 4.8597  time: 1.9292  data: 1.1543  max mem: 20296
Train Epoch: [0]  [ 550/3953]  eta: 1:55:19  lr: 0.000030  loss_ita: 4.3694  loss_itm: 0.7376  loss_lm: 5.2236  time: 1.9930  data: 1.2171  max mem: 20296
Train Epoch: [0]  [ 600/3953]  eta: 1:54:02  lr: 0.000030  loss_ita: 2.9743  loss_itm: 0.5544  loss_lm: 5.4362  time: 1.8208  data: 1.0442  max mem: 20296
Train Epoch: [0]  [ 650/3953]  eta: 1:52:08  lr: 0.000030  loss_ita: 2.6973  loss_itm: 0.5399  loss_lm: 4.8004  time: 1.6433  data: 0.8646  max mem: 20296
Train Epoch: [0]  [ 700/3953]  eta: 1:50:14  lr: 0.000030  loss_ita: 2.2276  loss_itm: 0.4729  loss_lm: 4.9860  time: 2.1718  data: 1.3967  max mem: 20296
Train Epoch: [0]  [ 750/3953]  eta: 1:49:06  lr: 0.000030  loss_ita: 3.1037  loss_itm: 0.4846  loss_lm: 4.7170  time: 2.1004  data: 1.3261  max mem: 20296
Train Epoch: [0]  [ 800/3953]  eta: 1:48:15  lr: 0.000030  loss_ita: 3.1335  loss_itm: 0.5674  loss_lm: 4.9683  time: 2.4807  data: 1.7064  max mem: 20296
Train Epoch: [0]  [ 850/3953]  eta: 1:47:17  lr: 0.000030  loss_ita: 2.0039  loss_itm: 0.5932  loss_lm: 5.1967  time: 2.1208  data: 1.3477  max mem: 20296
Train Epoch: [0]  [ 900/3953]  eta: 1:45:18  lr: 0.000030  loss_ita: 1.8569  loss_itm: 0.4629  loss_lm: 5.2801  time: 1.9617  data: 1.1849  max mem: 20296
Train Epoch: [0]  [ 950/3953]  eta: 1:43:55  lr: 0.000030  loss_ita: 2.6263  loss_itm: 0.5729  loss_lm: 4.7393  time: 1.9712  data: 1.1948  max mem: 20296
Train Epoch: [0]  [1000/3953]  eta: 1:41:30  lr: 0.000030  loss_ita: 3.7660  loss_itm: 0.7068  loss_lm: 4.4628  time: 1.9725  data: 1.1938  max mem: 20296
Train Epoch: [0]  [1050/3953]  eta: 1:39:29  lr: 0.000030  loss_ita: 2.5712  loss_itm: 0.5245  loss_lm: 5.2462  time: 2.0982  data: 1.3218  max mem: 20296
Train Epoch: [0]  [1100/3953]  eta: 1:37:35  lr: 0.000030  loss_ita: 3.6042  loss_itm: 0.5815  loss_lm: 5.0673  time: 1.9884  data: 1.2141  max mem: 20300
Train Epoch: [0]  [1150/3953]  eta: 1:35:06  lr: 0.000030  loss_ita: 2.8067  loss_itm: 0.5298  loss_lm: 4.9497  time: 1.5718  data: 0.7940  max mem: 20300
Train Epoch: [0]  [1200/3953]  eta: 1:33:18  lr: 0.000030  loss_ita: 3.3929  loss_itm: 0.6207  loss_lm: 4.5291  time: 1.8547  data: 1.0770  max mem: 20300
Train Epoch: [0]  [1250/3953]  eta: 1:31:27  lr: 0.000030  loss_ita: 2.6683  loss_itm: 0.5988  loss_lm: 4.5215  time: 1.8930  data: 1.1173  max mem: 20300
Train Epoch: [0]  [1300/3953]  eta: 1:29:18  lr: 0.000030  loss_ita: 3.0673  loss_itm: 0.6516  loss_lm: 4.3486  time: 1.6439  data: 0.8682  max mem: 20300
Train Epoch: [0]  [1350/3953]  eta: 1:27:26  lr: 0.000030  loss_ita: 3.0058  loss_itm: 0.5452  loss_lm: 4.7242  time: 1.7840  data: 1.0081  max mem: 20300
Train Epoch: [0]  [1400/3953]  eta: 1:25:33  lr: 0.000030  loss_ita: 2.9478  loss_itm: 0.4297  loss_lm: 4.6145  time: 1.6486  data: 0.8729  max mem: 20300
Train Epoch: [0]  [1450/3953]  eta: 1:23:21  lr: 0.000030  loss_ita: 3.0376  loss_itm: 0.6078  loss_lm: 4.7258  time: 1.8350  data: 1.0581  max mem: 20300
Train Epoch: [0]  [1500/3953]  eta: 1:21:46  lr: 0.000030  loss_ita: 2.3910  loss_itm: 0.5152  loss_lm: 4.8372  time: 2.0255  data: 1.2494  max mem: 20300
Train Epoch: [0]  [1550/3953]  eta: 1:19:49  lr: 0.000030  loss_ita: 2.4527  loss_itm: 0.5404  loss_lm: 4.6742  time: 1.8265  data: 1.0471  max mem: 20300
Train Epoch: [0]  [1600/3953]  eta: 1:17:56  lr: 0.000030  loss_ita: 2.5274  loss_itm: 0.4205  loss_lm: 4.5015  time: 1.6322  data: 0.8542  max mem: 20300
Train Epoch: [0]  [1650/3953]  eta: 1:16:01  lr: 0.000030  loss_ita: 3.0482  loss_itm: 0.5055  loss_lm: 4.3447  time: 1.7279  data: 0.9528  max mem: 20300
Train Epoch: [0]  [1700/3953]  eta: 1:14:39  lr: 0.000030  loss_ita: 2.2877  loss_itm: 0.5878  loss_lm: 4.8666  time: 2.2143  data: 1.4354  max mem: 20300
Train Epoch: [0]  [1750/3953]  eta: 1:12:57  lr: 0.000030  loss_ita: 2.7201  loss_itm: 0.4617  loss_lm: 4.4788  time: 2.1070  data: 1.3294  max mem: 20300
Train Epoch: [0]  [1800/3953]  eta: 1:11:27  lr: 0.000030  loss_ita: 2.0956  loss_itm: 0.7034  loss_lm: 4.0885  time: 2.0399  data: 1.2648  max mem: 20300
Train Epoch: [0]  [1850/3953]  eta: 1:10:06  lr: 0.000030  loss_ita: 2.4633  loss_itm: 0.4437  loss_lm: 4.7001  time: 2.2063  data: 1.4304  max mem: 20300
Train Epoch: [0]  [1900/3953]  eta: 1:08:16  lr: 0.000030  loss_ita: 3.3060  loss_itm: 0.5370  loss_lm: 4.3244  time: 1.7385  data: 0.9606  max mem: 20300
Train Epoch: [0]  [1950/3953]  eta: 1:06:42  lr: 0.000030  loss_ita: 2.8273  loss_itm: 0.4764  loss_lm: 5.1101  time: 1.7847  data: 1.0095  max mem: 20300
Train Epoch: [0]  [2000/3953]  eta: 1:04:45  lr: 0.000030  loss_ita: 2.2832  loss_itm: 0.4422  loss_lm: 4.8311  time: 1.6302  data: 0.8516  max mem: 20300
Train Epoch: [0]  [2050/3953]  eta: 1:02:57  lr: 0.000030  loss_ita: 3.5500  loss_itm: 0.5229  loss_lm: 4.4045  time: 1.9691  data: 1.1932  max mem: 20300
Train Epoch: [0]  [2100/3953]  eta: 1:01:17  lr: 0.000030  loss_ita: 2.5284  loss_itm: 0.3752  loss_lm: 4.0597  time: 2.1314  data: 1.3562  max mem: 20300
Train Epoch: [0]  [2150/3953]  eta: 0:59:39  lr: 0.000030  loss_ita: 2.3452  loss_itm: 0.5591  loss_lm: 4.6726  time: 1.8377  data: 1.0617  max mem: 20300
Train Epoch: [0]  [2200/3953]  eta: 0:58:03  lr: 0.000030  loss_ita: 2.4945  loss_itm: 0.4065  loss_lm: 4.1187  time: 2.3418  data: 1.5678  max mem: 20300
Train Epoch: [0]  [2250/3953]  eta: 0:56:44  lr: 0.000030  loss_ita: 2.8106  loss_itm: 0.5023  loss_lm: 4.2365  time: 2.4056  data: 1.6313  max mem: 20300
Train Epoch: [0]  [2300/3953]  eta: 0:55:04  lr: 0.000030  loss_ita: 2.4669  loss_itm: 0.4038  loss_lm: 4.5154  time: 1.8793  data: 1.1048  max mem: 20300
Train Epoch: [0]  [2350/3953]  eta: 0:53:13  lr: 0.000030  loss_ita: 3.5619  loss_itm: 0.5338  loss_lm: 4.9690  time: 1.5686  data: 0.7937  max mem: 20300
Train Epoch: [0]  [2400/3953]  eta: 0:51:30  lr: 0.000030  loss_ita: 3.1594  loss_itm: 0.6094  loss_lm: 4.2884  time: 2.0266  data: 1.2512  max mem: 20300
Train Epoch: [0]  [2450/3953]  eta: 0:49:55  lr: 0.000030  loss_ita: 2.4620  loss_itm: 0.4161  loss_lm: 4.5222  time: 2.2521  data: 1.4765  max mem: 20300
Train Epoch: [0]  [2500/3953]  eta: 0:48:22  lr: 0.000030  loss_ita: 2.9344  loss_itm: 0.5494  loss_lm: 4.0170  time: 1.9274  data: 1.1511  max mem: 20301
Train Epoch: [0]  [2550/3953]  eta: 0:46:42  lr: 0.000030  loss_ita: 3.4261  loss_itm: 0.5242  loss_lm: 4.0822  time: 1.8339  data: 1.0563  max mem: 20302
Train Epoch: [0]  [2600/3953]  eta: 0:45:02  lr: 0.000030  loss_ita: 2.4188  loss_itm: 0.5710  loss_lm: 3.8472  time: 1.9390  data: 1.1638  max mem: 20302
Train Epoch: [0]  [2650/3953]  eta: 0:43:21  lr: 0.000030  loss_ita: 2.3911  loss_itm: 0.4063  loss_lm: 4.9453  time: 2.0067  data: 1.2322  max mem: 20302
Train Epoch: [0]  [2700/3953]  eta: 0:41:35  lr: 0.000030  loss_ita: 2.6064  loss_itm: 0.4687  loss_lm: 4.2444  time: 1.5921  data: 0.8166  max mem: 20302
Train Epoch: [0]  [2750/3953]  eta: 0:39:55  lr: 0.000030  loss_ita: 2.8207  loss_itm: 0.3947  loss_lm: 4.4077  time: 2.0439  data: 1.2680  max mem: 20302
Train Epoch: [0]  [2800/3953]  eta: 0:38:20  lr: 0.000030  loss_ita: 3.2651  loss_itm: 0.4386  loss_lm: 3.8831  time: 1.9397  data: 1.1648  max mem: 20302
Train Epoch: [0]  [2850/3953]  eta: 0:36:40  lr: 0.000030  loss_ita: 2.7006  loss_itm: 0.3046  loss_lm: 3.7728  time: 1.4910  data: 0.7144  max mem: 20302
Train Epoch: [0]  [2900/3953]  eta: 0:35:03  lr: 0.000030  loss_ita: 3.5662  loss_itm: 0.4478  loss_lm: 4.1964  time: 2.1811  data: 1.4069  max mem: 20302
Train Epoch: [0]  [2950/3953]  eta: 0:33:19  lr: 0.000030  loss_ita: 2.6368  loss_itm: 0.4834  loss_lm: 4.2256  time: 1.7082  data: 0.9323  max mem: 20302
Train Epoch: [0]  [3000/3953]  eta: 0:31:34  lr: 0.000030  loss_ita: 2.9730  loss_itm: 0.5850  loss_lm: 3.7768  time: 1.7261  data: 0.9471  max mem: 20302
Train Epoch: [0]  [3050/3953]  eta: 0:29:55  lr: 0.000030  loss_ita: 3.5921  loss_itm: 0.5080  loss_lm: 4.6128  time: 2.1385  data: 1.3626  max mem: 20302
Train Epoch: [0]  [3100/3953]  eta: 0:28:18  lr: 0.000030  loss_ita: 3.0517  loss_itm: 0.3779  loss_lm: 4.2530  time: 2.3846  data: 1.6106  max mem: 20302
Train Epoch: [0]  [3150/3953]  eta: 0:26:37  lr: 0.000030  loss_ita: 3.8391  loss_itm: 0.6384  loss_lm: 4.1275  time: 1.5995  data: 0.8235  max mem: 20302
Train Epoch: [0]  [3200/3953]  eta: 0:24:58  lr: 0.000030  loss_ita: 2.8371  loss_itm: 0.3627  loss_lm: 3.9917  time: 2.5253  data: 1.7487  max mem: 20302
Train Epoch: [0]  [3250/3953]  eta: 0:23:17  lr: 0.000030  loss_ita: 2.6241  loss_itm: 0.4156  loss_lm: 4.5052  time: 1.7683  data: 0.9920  max mem: 20302
Train Epoch: [0]  [3300/3953]  eta: 0:21:34  lr: 0.000030  loss_ita: 3.1904  loss_itm: 0.4335  loss_lm: 4.2514  time: 1.6493  data: 0.8726  max mem: 20304
Train Epoch: [0]  [3350/3953]  eta: 0:19:56  lr: 0.000030  loss_ita: 2.6643  loss_itm: 0.5484  loss_lm: 4.1398  time: 2.2700  data: 1.4931  max mem: 20304
Train Epoch: [0]  [3400/3953]  eta: 0:18:15  lr: 0.000030  loss_ita: 2.6825  loss_itm: 0.2706  loss_lm: 4.6001  time: 2.0480  data: 1.2713  max mem: 20304
Train Epoch: [0]  [3450/3953]  eta: 0:16:34  lr: 0.000030  loss_ita: 3.7879  loss_itm: 0.4725  loss_lm: 4.1443  time: 1.4648  data: 0.6883  max mem: 20304
Train Epoch: [0]  [3500/3953]  eta: 0:14:55  lr: 0.000030  loss_ita: 3.0078  loss_itm: 0.3993  loss_lm: 4.6205  time: 1.9290  data: 1.1518  max mem: 20304
Train Epoch: [0]  [3550/3953]  eta: 0:13:17  lr: 0.000030  loss_ita: 4.1932  loss_itm: 0.5363  loss_lm: 4.5000  time: 1.8733  data: 1.0970  max mem: 20304
Train Epoch: [0]  [3600/3953]  eta: 0:11:39  lr: 0.000030  loss_ita: 3.3140  loss_itm: 0.5340  loss_lm: 3.8412  time: 1.8300  data: 1.0518  max mem: 20304
Train Epoch: [0]  [3650/3953]  eta: 0:10:00  lr: 0.000030  loss_ita: 3.0728  loss_itm: 0.5105  loss_lm: 4.3679  time: 2.3133  data: 1.5382  max mem: 20304
Train Epoch: [0]  [3700/3953]  eta: 0:08:20  lr: 0.000030  loss_ita: 3.3875  loss_itm: 0.5931  loss_lm: 3.7778  time: 1.6701  data: 0.8917  max mem: 20304
Train Epoch: [0]  [3750/3953]  eta: 0:06:40  lr: 0.000030  loss_ita: 3.0421  loss_itm: 0.3828  loss_lm: 3.8849  time: 1.7659  data: 0.9887  max mem: 20304
Train Epoch: [0]  [3800/3953]  eta: 0:05:01  lr: 0.000030  loss_ita: 2.9650  loss_itm: 0.4742  loss_lm: 3.9856  time: 1.9514  data: 1.1762  max mem: 20304
Train Epoch: [0]  [3850/3953]  eta: 0:03:22  lr: 0.000030  loss_ita: 2.8570  loss_itm: 0.5188  loss_lm: 4.3087  time: 1.7742  data: 0.9977  max mem: 20304
Train Epoch: [0]  [3900/3953]  eta: 0:01:44  lr: 0.000030  loss_ita: 3.4456  loss_itm: 0.5055  loss_lm: 4.2757  time: 1.8582  data: 1.0798  max mem: 20304
Train Epoch: [0]  [3950/3953]  eta: 0:00:05  lr: 0.000030  loss_ita: 3.4398  loss_itm: 0.6047  loss_lm: 3.8175  time: 2.2087  data: 1.4331  max mem: 20304
Train Epoch: [0]  [3952/3953]  eta: 0:00:01  lr: 0.000030  loss_ita: 2.1393  loss_itm: 0.6024  loss_lm: 4.8995  time: 2.1990  data: 1.4234  max mem: 20304
Train Epoch: [0] Total time: 2:09:44 (1.9693 s / it)
Averaged stats: lr: 0.0000  loss_ita: 3.0048  loss_itm: 0.5145  loss_lm: 4.7105
Train Epoch: [1]  [   0/3953]  eta: 10:05:56  lr: 0.000027  loss_ita: 2.1431  loss_itm: 0.3779  loss_lm: 4.1477  time: 9.1971  data: 8.4159  max mem: 20304
Train Epoch: [1]  [  50/3953]  eta: 2:23:57  lr: 0.000027  loss_ita: 2.9496  loss_itm: 0.4450  loss_lm: 4.1801  time: 1.6644  data: 0.8874  max mem: 20304
Train Epoch: [1]  [ 100/3953]  eta: 2:10:59  lr: 0.000027  loss_ita: 3.2918  loss_itm: 0.6952  loss_lm: 4.8702  time: 1.9420  data: 1.1640  max mem: 20304
Train Epoch: [1]  [ 150/3953]  eta: 2:10:26  lr: 0.000027  loss_ita: 2.7740  loss_itm: 0.3698  loss_lm: 3.9383  time: 1.8649  data: 1.0891  max mem: 20305
Train Epoch: [1]  [ 200/3953]  eta: 2:03:49  lr: 0.000027  loss_ita: 3.5601  loss_itm: 0.6226  loss_lm: 4.5565  time: 1.7972  data: 1.0198  max mem: 20305
Train Epoch: [1]  [ 250/3953]  eta: 2:02:02  lr: 0.000027  loss_ita: 2.2879  loss_itm: 0.3017  loss_lm: 4.6121  time: 2.3617  data: 1.5830  max mem: 20305
Train Epoch: [1]  [ 300/3953]  eta: 1:59:06  lr: 0.000027  loss_ita: 2.6169  loss_itm: 0.4633  loss_lm: 3.9954  time: 1.9280  data: 1.1509  max mem: 20305
Train Epoch: [1]  [ 350/3953]  eta: 1:56:57  lr: 0.000027  loss_ita: 2.4980  loss_itm: 0.3758  loss_lm: 4.0920  time: 1.8461  data: 1.0691  max mem: 20305
Train Epoch: [1]  [ 400/3953]  eta: 1:54:03  lr: 0.000027  loss_ita: 3.6743  loss_itm: 0.4758  loss_lm: 4.4493  time: 1.6899  data: 0.9128  max mem: 20305
Train Epoch: [1]  [ 450/3953]  eta: 1:52:35  lr: 0.000027  loss_ita: 3.0525  loss_itm: 0.4473  loss_lm: 4.3434  time: 1.6116  data: 0.8346  max mem: 20305
Train Epoch: [1]  [ 500/3953]  eta: 1:50:42  lr: 0.000027  loss_ita: 3.1871  loss_itm: 0.5556  loss_lm: 3.9172  time: 1.6680  data: 0.8916  max mem: 20305
Train Epoch: [1]  [ 550/3953]  eta: 1:48:14  lr: 0.000027  loss_ita: 3.0422  loss_itm: 0.4670  loss_lm: 4.2235  time: 1.8170  data: 1.0394  max mem: 20305
Train Epoch: [1]  [ 600/3953]  eta: 1:47:02  lr: 0.000027  loss_ita: 2.7103  loss_itm: 0.4222  loss_lm: 4.1355  time: 1.9685  data: 1.1944  max mem: 20305
Train Epoch: [1]  [ 650/3953]  eta: 1:45:46  lr: 0.000027  loss_ita: 2.8341  loss_itm: 0.2480  loss_lm: 3.9154  time: 2.1343  data: 1.3581  max mem: 20305
Train Epoch: [1]  [ 700/3953]  eta: 1:44:16  lr: 0.000027  loss_ita: 3.9776  loss_itm: 0.6543  loss_lm: 4.3684  time: 2.2532  data: 1.4786  max mem: 20305
Train Epoch: [1]  [ 750/3953]  eta: 1:42:30  lr: 0.000027  loss_ita: 4.0153  loss_itm: 0.4734  loss_lm: 4.3709  time: 1.7546  data: 0.9803  max mem: 20305
Train Epoch: [1]  [ 800/3953]  eta: 1:40:05  lr: 0.000027  loss_ita: 3.8702  loss_itm: 0.6086  loss_lm: 4.1363  time: 1.8195  data: 1.0404  max mem: 20305
Train Epoch: [1]  [ 850/3953]  eta: 1:37:53  lr: 0.000027  loss_ita: 2.7814  loss_itm: 0.3003  loss_lm: 4.1871  time: 1.5812  data: 0.8058  max mem: 20305
Train Epoch: [1]  [ 900/3953]  eta: 1:36:06  lr: 0.000027  loss_ita: 3.0095  loss_itm: 0.3701  loss_lm: 3.9837  time: 1.7926  data: 1.0172  max mem: 20305
Train Epoch: [1]  [ 950/3953]  eta: 1:35:10  lr: 0.000027  loss_ita: 4.6208  loss_itm: 0.6019  loss_lm: 4.6113  time: 2.3074  data: 1.5308  max mem: 20305
Train Epoch: [1]  [1000/3953]  eta: 1:33:35  lr: 0.000027  loss_ita: 3.3758  loss_itm: 0.4409  loss_lm: 4.1514  time: 1.9240  data: 1.1497  max mem: 20305
Train Epoch: [1]  [1050/3953]  eta: 1:32:20  lr: 0.000027  loss_ita: 2.9272  loss_itm: 0.3811  loss_lm: 4.1492  time: 2.3527  data: 1.5759  max mem: 20305
Train Epoch: [1]  [1100/3953]  eta: 1:31:40  lr: 0.000027  loss_ita: 3.2180  loss_itm: 0.4791  loss_lm: 4.3820  time: 1.8438  data: 1.0670  max mem: 20305
Train Epoch: [1]  [1150/3953]  eta: 1:30:21  lr: 0.000027  loss_ita: 2.6225  loss_itm: 0.5458  loss_lm: 3.8995  time: 2.0137  data: 1.2363  max mem: 20305
Train Epoch: [1]  [1200/3953]  eta: 1:29:02  lr: 0.000027  loss_ita: 3.0781  loss_itm: 0.3598  loss_lm: 4.0997  time: 2.3433  data: 1.5672  max mem: 20305
Train Epoch: [1]  [1250/3953]  eta: 1:27:38  lr: 0.000027  loss_ita: 3.5207  loss_itm: 0.3659  loss_lm: 4.2011  time: 2.0459  data: 1.2697  max mem: 20305
Train Epoch: [1]  [1300/3953]  eta: 1:26:39  lr: 0.000027  loss_ita: 2.5447  loss_itm: 0.5489  loss_lm: 4.6792  time: 2.0923  data: 1.3189  max mem: 20305
Train Epoch: [1]  [1350/3953]  eta: 1:24:57  lr: 0.000027  loss_ita: 3.3838  loss_itm: 0.3382  loss_lm: 4.2188  time: 1.7309  data: 0.9560  max mem: 20305
Train Epoch: [1]  [1400/3953]  eta: 1:23:10  lr: 0.000027  loss_ita: 3.4421  loss_itm: 0.4858  loss_lm: 4.6091  time: 1.6595  data: 0.8848  max mem: 20305
Train Epoch: [1]  [1450/3953]  eta: 1:21:36  lr: 0.000027  loss_ita: 3.0857  loss_itm: 0.3054  loss_lm: 3.9064  time: 1.9141  data: 1.1394  max mem: 20305
Train Epoch: [1]  [1500/3953]  eta: 1:19:57  lr: 0.000027  loss_ita: 3.7406  loss_itm: 0.4679  loss_lm: 4.3366  time: 1.7829  data: 1.0059  max mem: 20305
Train Epoch: [1]  [1550/3953]  eta: 1:18:26  lr: 0.000027  loss_ita: 2.4035  loss_itm: 0.2920  loss_lm: 4.1976  time: 2.3407  data: 1.5639  max mem: 20305
Train Epoch: [1]  [1600/3953]  eta: 1:16:48  lr: 0.000027  loss_ita: 3.7864  loss_itm: 0.4788  loss_lm: 4.0716  time: 1.7901  data: 1.0124  max mem: 20305
Train Epoch: [1]  [1650/3953]  eta: 1:15:03  lr: 0.000027  loss_ita: 2.5527  loss_itm: 0.3084  loss_lm: 3.9953  time: 1.7707  data: 0.9933  max mem: 20305
Train Epoch: [1]  [1700/3953]  eta: 1:13:15  lr: 0.000027  loss_ita: 3.6100  loss_itm: 0.4500  loss_lm: 3.8228  time: 1.6534  data: 0.8731  max mem: 20305
Train Epoch: [1]  [1750/3953]  eta: 1:11:51  lr: 0.000027  loss_ita: 3.0244  loss_itm: 0.4306  loss_lm: 4.0045  time: 1.8636  data: 1.0854  max mem: 20305
Train Epoch: [1]  [1800/3953]  eta: 1:10:15  lr: 0.000027  loss_ita: 2.9238  loss_itm: 0.2825  loss_lm: 4.0238  time: 2.2822  data: 1.5059  max mem: 20305
Train Epoch: [1]  [1850/3953]  eta: 1:08:28  lr: 0.000027  loss_ita: 2.7561  loss_itm: 0.3506  loss_lm: 3.5866  time: 1.9461  data: 1.1693  max mem: 20305
Train Epoch: [1]  [1900/3953]  eta: 1:06:50  lr: 0.000027  loss_ita: 2.4334  loss_itm: 0.1505  loss_lm: 4.0565  time: 2.1994  data: 1.4249  max mem: 20305
Train Epoch: [1]  [1950/3953]  eta: 1:05:26  lr: 0.000027  loss_ita: 3.3195  loss_itm: 0.4084  loss_lm: 4.0642  time: 1.8663  data: 1.0902  max mem: 20306
Train Epoch: [1]  [2000/3953]  eta: 1:03:52  lr: 0.000027  loss_ita: 3.4605  loss_itm: 0.6173  loss_lm: 3.7258  time: 2.1305  data: 1.3543  max mem: 20306
Train Epoch: [1]  [2050/3953]  eta: 1:02:15  lr: 0.000027  loss_ita: 2.5195  loss_itm: 0.2114  loss_lm: 4.0395  time: 1.5612  data: 0.7837  max mem: 20306
Train Epoch: [1]  [2100/3953]  eta: 1:00:36  lr: 0.000027  loss_ita: 3.8921  loss_itm: 0.4134  loss_lm: 4.2363  time: 1.8907  data: 1.1171  max mem: 20311
Train Epoch: [1]  [2150/3953]  eta: 0:59:02  lr: 0.000027  loss_ita: 3.4625  loss_itm: 0.5776  loss_lm: 4.4677  time: 2.5125  data: 1.7372  max mem: 20311
Train Epoch: [1]  [2200/3953]  eta: 0:57:29  lr: 0.000027  loss_ita: 3.1884  loss_itm: 0.4029  loss_lm: 3.8019  time: 1.9637  data: 1.1899  max mem: 20311
Train Epoch: [1]  [2250/3953]  eta: 0:55:49  lr: 0.000027  loss_ita: 3.1506  loss_itm: 0.4201  loss_lm: 3.7549  time: 1.9273  data: 1.1492  max mem: 20311
Train Epoch: [1]  [2300/3953]  eta: 0:54:13  lr: 0.000027  loss_ita: 3.6736  loss_itm: 0.4852  loss_lm: 3.5225  time: 1.9884  data: 1.2116  max mem: 20311
Train Epoch: [1]  [2350/3953]  eta: 0:52:30  lr: 0.000027  loss_ita: 3.7489  loss_itm: 0.3707  loss_lm: 3.8245  time: 1.5718  data: 0.7935  max mem: 20311
Train Epoch: [1]  [2400/3953]  eta: 0:50:59  lr: 0.000027  loss_ita: 3.4283  loss_itm: 0.4123  loss_lm: 3.8714  time: 2.1870  data: 1.4098  max mem: 20311
Train Epoch: [1]  [2450/3953]  eta: 0:49:18  lr: 0.000027  loss_ita: 3.5210  loss_itm: 0.5515  loss_lm: 3.8625  time: 1.8132  data: 1.0354  max mem: 20311
Train Epoch: [1]  [2500/3953]  eta: 0:47:38  lr: 0.000027  loss_ita: 2.8452  loss_itm: 0.3724  loss_lm: 4.0627  time: 2.3689  data: 1.5907  max mem: 20311
Train Epoch: [1]  [2550/3953]  eta: 0:45:58  lr: 0.000027  loss_ita: 2.9945  loss_itm: 0.2799  loss_lm: 3.9188  time: 2.0812  data: 1.3027  max mem: 20311
Train Epoch: [1]  [2600/3953]  eta: 0:44:24  lr: 0.000027  loss_ita: 3.3701  loss_itm: 0.5645  loss_lm: 4.2597  time: 1.8062  data: 1.0290  max mem: 20311
Train Epoch: [1]  [2650/3953]  eta: 0:42:46  lr: 0.000027  loss_ita: 3.5091  loss_itm: 0.3903  loss_lm: 3.5887  time: 1.6527  data: 0.8784  max mem: 20311
Train Epoch: [1]  [2700/3953]  eta: 0:41:07  lr: 0.000027  loss_ita: 3.4309  loss_itm: 0.4322  loss_lm: 3.6905  time: 2.0273  data: 1.2525  max mem: 20311
Train Epoch: [1]  [2750/3953]  eta: 0:39:27  lr: 0.000027  loss_ita: 3.1915  loss_itm: 0.3992  loss_lm: 3.9232  time: 1.9046  data: 1.1291  max mem: 20311
Train Epoch: [1]  [2800/3953]  eta: 0:37:42  lr: 0.000027  loss_ita: 3.6479  loss_itm: 0.4925  loss_lm: 3.7857  time: 1.5362  data: 0.7583  max mem: 20311
Train Epoch: [1]  [2850/3953]  eta: 0:36:03  lr: 0.000027  loss_ita: 2.8103  loss_itm: 0.4122  loss_lm: 3.6711  time: 1.8024  data: 1.0258  max mem: 20311
Train Epoch: [1]  [2900/3953]  eta: 0:34:23  lr: 0.000027  loss_ita: 3.5249  loss_itm: 0.5118  loss_lm: 3.9161  time: 1.8717  data: 1.0954  max mem: 20311
Train Epoch: [1]  [2950/3953]  eta: 0:32:46  lr: 0.000027  loss_ita: 2.7830  loss_itm: 0.5488  loss_lm: 3.9524  time: 2.5839  data: 1.8092  max mem: 20311
Train Epoch: [1]  [3000/3953]  eta: 0:31:10  lr: 0.000027  loss_ita: 3.0567  loss_itm: 0.2529  loss_lm: 3.8531  time: 1.9566  data: 1.1808  max mem: 20311
Train Epoch: [1]  [3050/3953]  eta: 0:29:29  lr: 0.000027  loss_ita: 3.0219  loss_itm: 0.2163  loss_lm: 4.2533  time: 1.6749  data: 0.8957  max mem: 20311
Train Epoch: [1]  [3100/3953]  eta: 0:27:48  lr: 0.000027  loss_ita: 3.7501  loss_itm: 0.6909  loss_lm: 3.5623  time: 2.0006  data: 1.2240  max mem: 20311
Train Epoch: [1]  [3150/3953]  eta: 0:26:12  lr: 0.000027  loss_ita: 3.6159  loss_itm: 0.4563  loss_lm: 4.2540  time: 2.1170  data: 1.3404  max mem: 20311
Train Epoch: [1]  [3200/3953]  eta: 0:24:36  lr: 0.000027  loss_ita: 3.2872  loss_itm: 0.5616  loss_lm: 4.0278  time: 2.4346  data: 1.6570  max mem: 20311
Train Epoch: [1]  [3250/3953]  eta: 0:22:58  lr: 0.000027  loss_ita: 3.1410  loss_itm: 0.2798  loss_lm: 4.0329  time: 2.3857  data: 1.6109  max mem: 20311
Train Epoch: [1]  [3300/3953]  eta: 0:21:21  lr: 0.000027  loss_ita: 2.7684  loss_itm: 0.2092  loss_lm: 4.0161  time: 2.2801  data: 1.5050  max mem: 20311
Train Epoch: [1]  [3350/3953]  eta: 0:19:43  lr: 0.000027  loss_ita: 2.5272  loss_itm: 0.1848  loss_lm: 4.0633  time: 2.1504  data: 1.3738  max mem: 20311
Train Epoch: [1]  [3400/3953]  eta: 0:18:05  lr: 0.000027  loss_ita: 3.4532  loss_itm: 0.3594  loss_lm: 3.9674  time: 2.0132  data: 1.2371  max mem: 20311
Train Epoch: [1]  [3450/3953]  eta: 0:16:27  lr: 0.000027  loss_ita: 4.1532  loss_itm: 0.4803  loss_lm: 3.9638  time: 2.3079  data: 1.5339  max mem: 20311
Train Epoch: [1]  [3500/3953]  eta: 0:14:48  lr: 0.000027  loss_ita: 2.9799  loss_itm: 0.2479  loss_lm: 4.7735  time: 2.1357  data: 1.3589  max mem: 20311
Train Epoch: [1]  [3550/3953]  eta: 0:13:11  lr: 0.000027  loss_ita: 3.1851  loss_itm: 0.3005  loss_lm: 3.6338  time: 2.5909  data: 1.8121  max mem: 20311
Train Epoch: [1]  [3600/3953]  eta: 0:11:34  lr: 0.000027  loss_ita: 4.1898  loss_itm: 0.5190  loss_lm: 3.6587  time: 2.2254  data: 1.4506  max mem: 20311
Train Epoch: [1]  [3650/3953]  eta: 0:09:55  lr: 0.000027  loss_ita: 3.0906  loss_itm: 0.3352  loss_lm: 3.7396  time: 2.2091  data: 1.4328  max mem: 20311
Train Epoch: [1]  [3700/3953]  eta: 0:08:17  lr: 0.000027  loss_ita: 3.2566  loss_itm: 0.3731  loss_lm: 3.7460  time: 2.2035  data: 1.4274  max mem: 20311
Train Epoch: [1]  [3750/3953]  eta: 0:06:39  lr: 0.000027  loss_ita: 2.7741  loss_itm: 0.2973  loss_lm: 3.7858  time: 2.0131  data: 1.2373  max mem: 20311
Train Epoch: [1]  [3800/3953]  eta: 0:05:00  lr: 0.000027  loss_ita: 3.0894  loss_itm: 0.4051  loss_lm: 4.0593  time: 2.0973  data: 1.3186  max mem: 20311
Train Epoch: [1]  [3850/3953]  eta: 0:03:22  lr: 0.000027  loss_ita: 3.5415  loss_itm: 0.2903  loss_lm: 4.0624  time: 2.4276  data: 1.6513  max mem: 20311
Train Epoch: [1]  [3900/3953]  eta: 0:01:44  lr: 0.000027  loss_ita: 3.0496  loss_itm: 0.3090  loss_lm: 4.2013  time: 1.9189  data: 1.1416  max mem: 20311
Train Epoch: [1]  [3950/3953]  eta: 0:00:05  lr: 0.000027  loss_ita: 3.0014  loss_itm: 0.3559  loss_lm: 3.7519  time: 1.8975  data: 1.1222  max mem: 20311
Train Epoch: [1]  [3952/3953]  eta: 0:00:01  lr: 0.000027  loss_ita: 3.2391  loss_itm: 0.3664  loss_lm: 3.7544  time: 2.2445  data: 1.4698  max mem: 20311
Train Epoch: [1] Total time: 2:09:23 (1.9640 s / it)
Averaged stats: lr: 0.0000  loss_ita: 3.2710  loss_itm: 0.4169  loss_lm: 3.9815
Train Epoch: [2]  [   0/3953]  eta: 6:21:00  lr: 0.000024  loss_ita: 3.0606  loss_itm: 0.2246  loss_lm: 3.4691  time: 5.7831  data: 5.0075  max mem: 20311
Train Epoch: [2]  [  50/3953]  eta: 2:19:23  lr: 0.000024  loss_ita: 3.7188  loss_itm: 0.4149  loss_lm: 3.6521  time: 2.2980  data: 1.5236  max mem: 20311
Train Epoch: [2]  [ 100/3953]  eta: 2:15:40  lr: 0.000024  loss_ita: 3.9327  loss_itm: 0.3799  loss_lm: 3.9898  time: 1.7703  data: 0.9934  max mem: 20311
Train Epoch: [2]  [ 150/3953]  eta: 2:10:02  lr: 0.000024  loss_ita: 3.4397  loss_itm: 0.3094  loss_lm: 4.1774  time: 1.9489  data: 1.1720  max mem: 20311
Train Epoch: [2]  [ 200/3953]  eta: 2:06:07  lr: 0.000024  loss_ita: 3.1086  loss_itm: 0.3010  loss_lm: 3.8538  time: 1.7249  data: 0.9476  max mem: 20311
Train Epoch: [2]  [ 250/3953]  eta: 2:02:09  lr: 0.000024  loss_ita: 3.2181  loss_itm: 0.3221  loss_lm: 3.7972  time: 1.6238  data: 0.8454  max mem: 20311
Train Epoch: [2]  [ 300/3953]  eta: 1:59:48  lr: 0.000024  loss_ita: 4.3387  loss_itm: 0.3769  loss_lm: 3.9292  time: 1.8792  data: 1.1043  max mem: 20311
Train Epoch: [2]  [ 350/3953]  eta: 2:02:14  lr: 0.000024  loss_ita: 3.7117  loss_itm: 0.6750  loss_lm: 3.7364  time: 2.6714  data: 1.8963  max mem: 20311
Train Epoch: [2]  [ 400/3953]  eta: 2:00:26  lr: 0.000024  loss_ita: 3.5578  loss_itm: 0.3631  loss_lm: 3.7476  time: 2.4357  data: 1.6611  max mem: 20311
Train Epoch: [2]  [ 450/3953]  eta: 1:57:11  lr: 0.000024  loss_ita: 2.7177  loss_itm: 0.1517  loss_lm: 3.6788  time: 1.9412  data: 1.1638  max mem: 20311
Train Epoch: [2]  [ 500/3953]  eta: 1:55:33  lr: 0.000024  loss_ita: 3.4787  loss_itm: 0.4630  loss_lm: 3.5923  time: 1.8297  data: 1.0530  max mem: 20311
Train Epoch: [2]  [ 550/3953]  eta: 1:55:10  lr: 0.000024  loss_ita: 3.2497  loss_itm: 0.6358  loss_lm: 3.3024  time: 2.7189  data: 1.9433  max mem: 20311
Train Epoch: [2]  [ 600/3953]  eta: 1:53:15  lr: 0.000024  loss_ita: 2.5687  loss_itm: 0.2419  loss_lm: 3.4554  time: 2.0108  data: 1.2307  max mem: 20311
Train Epoch: [2]  [ 650/3953]  eta: 1:51:16  lr: 0.000024  loss_ita: 2.3649  loss_itm: 0.2865  loss_lm: 3.8689  time: 1.8893  data: 1.1129  max mem: 20311
Train Epoch: [2]  [ 700/3953]  eta: 1:47:59  lr: 0.000024  loss_ita: 3.4199  loss_itm: 0.4738  loss_lm: 3.6771  time: 1.6023  data: 0.8256  max mem: 20311
Train Epoch: [2]  [ 750/3953]  eta: 1:44:58  lr: 0.000024  loss_ita: 2.9363  loss_itm: 0.3584  loss_lm: 3.8796  time: 1.4630  data: 0.6853  max mem: 20311
Train Epoch: [2]  [ 800/3953]  eta: 1:42:04  lr: 0.000024  loss_ita: 2.8888  loss_itm: 0.2829  loss_lm: 4.1069  time: 1.3721  data: 0.5926  max mem: 20311
Train Epoch: [2]  [ 850/3953]  eta: 1:41:33  lr: 0.000024  loss_ita: 2.6084  loss_itm: 0.5016  loss_lm: 3.9604  time: 2.3716  data: 1.5952  max mem: 20311
Train Epoch: [2]  [ 900/3953]  eta: 1:40:05  lr: 0.000024  loss_ita: 3.9108  loss_itm: 0.5311  loss_lm: 4.3328  time: 2.2981  data: 1.5209  max mem: 20311
Train Epoch: [2]  [ 950/3953]  eta: 1:37:53  lr: 0.000024  loss_ita: 3.6183  loss_itm: 0.3714  loss_lm: 3.8555  time: 1.6909  data: 0.9108  max mem: 20311
Train Epoch: [2]  [1000/3953]  eta: 1:36:32  lr: 0.000024  loss_ita: 2.7648  loss_itm: 0.1211  loss_lm: 3.3575  time: 2.0430  data: 1.2675  max mem: 20311
Traceback (most recent call last):
  File "pretrain.py", line 176, in <module>
    main(args, config)
  File "pretrain.py", line 133, in main
    train_stats = train(model, data_loader, optimizer, epoch, device, config) 
  File "pretrain.py", line 48, in train
    for i, (image, caption) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):
  File "/home/monajati/main/med/blip/utils.py", line 156, in log_every
    for obj in iterable:
  File "/home/monajati/miniconda3/envs/vlt6/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 652, in __next__
    data = self._next_data()
  File "/home/monajati/miniconda3/envs/vlt6/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    return self._process_data(data)
  File "/home/monajati/miniconda3/envs/vlt6/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1373, in _process_data
    data.reraise()
  File "/home/monajati/miniconda3/envs/vlt6/lib/python3.7/site-packages/torch/_utils.py", line 461, in reraise
    raise exception
google.api_core.exceptions.InternalServerError: 500 Caught InternalServerError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/home/monajati/miniconda3/envs/vlt6/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 50, in error_remapped_callable
    return callable_(*args, **kwargs)
  File "/home/monajati/miniconda3/envs/vlt6/lib/python3.7/site-packages/grpc/_channel.py", line 946, in __call__
    return _end_unary_response_blocking(state, call, False, None)
  File "/home/monajati/miniconda3/envs/vlt6/lib/python3.7/site-packages/grpc/_channel.py", line 849, in _end_unary_response_blocking
    raise _InactiveRpcError(state)
grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
	status = StatusCode.INTERNAL
	details = "Received RST_STREAM with error code 2"
	debug_error_string = "{"created":"@1658057333.524605997","description":"Error received from peer ipv4:172.217.14.106:443","file":"src/core/lib/surface/call.cc","file_line":966,"grpc_message":"Received RST_STREAM with error code 2","grpc_status":13}"
>

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/monajati/miniconda3/envs/vlt6/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py", line 302, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/monajati/miniconda3/envs/vlt6/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 49, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/monajati/miniconda3/envs/vlt6/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 49, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/monajati/main/med/blip/data/pretrain_dataset.py", line 92, in __getitem__
    response = client.text_detection(image=image)
  File "/home/monajati/miniconda3/envs/vlt6/lib/python3.7/site-packages/google/cloud/vision_helpers/decorators.py", line 113, in inner
    request, retry=retry, timeout=timeout, metadata=metadata
  File "/home/monajati/miniconda3/envs/vlt6/lib/python3.7/site-packages/google/cloud/vision_helpers/__init__.py", line 77, in annotate_image
    requests=[request], retry=retry, timeout=timeout, metadata=metadata
  File "/home/monajati/miniconda3/envs/vlt6/lib/python3.7/site-packages/google/cloud/vision_v1/services/image_annotator/client.py", line 544, in batch_annotate_images
    metadata=metadata,
  File "/home/monajati/miniconda3/envs/vlt6/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 154, in __call__
    return wrapped_func(*args, **kwargs)
  File "/home/monajati/miniconda3/envs/vlt6/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 52, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.InternalServerError: 500 Received RST_STREAM with error code 2

ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 1379272) of binary: /home/monajati/miniconda3/envs/vlt6/bin/python
Traceback (most recent call last):
  File "/home/monajati/miniconda3/envs/vlt6/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/monajati/miniconda3/envs/vlt6/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/monajati/miniconda3/envs/vlt6/lib/python3.7/site-packages/torch/distributed/run.py", line 765, in <module>
    main()
  File "/home/monajati/miniconda3/envs/vlt6/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/monajati/miniconda3/envs/vlt6/lib/python3.7/site-packages/torch/distributed/run.py", line 761, in main
    run(args)
  File "/home/monajati/miniconda3/envs/vlt6/lib/python3.7/site-packages/torch/distributed/run.py", line 755, in run
    )(*cmd_args)
  File "/home/monajati/miniconda3/envs/vlt6/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/monajati/miniconda3/envs/vlt6/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 247, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-07-17_04:29:10
  host      : pluslab-a6000
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1379272)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
